{"cells":[{"cell_type":"code","source":["pip install confluent-kafka"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"39ee68d4-5148-485b-918f-bdaffe5680d2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting confluent-kafka\n  Downloading confluent_kafka-1.5.0-cp37-cp37m-manylinux1_x86_64.whl (8.1 MB)\nInstalling collected packages: confluent-kafka\nSuccessfully installed confluent-kafka-1.5.0\nPython interpreter will be restarted.\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting confluent-kafka\n  Downloading confluent_kafka-1.5.0-cp37-cp37m-manylinux1_x86_64.whl (8.1 MB)\nInstalling collected packages: confluent-kafka\nSuccessfully installed confluent-kafka-1.5.0\nPython interpreter will be restarted.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["pip install kafka-python"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"519a2bef-7174-42bf-a159-558dc364d6e3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting kafka-python\n  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\nInstalling collected packages: kafka-python\nSuccessfully installed kafka-python-2.0.2\nPython interpreter will be restarted.\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting kafka-python\n  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\nInstalling collected packages: kafka-python\nSuccessfully installed kafka-python-2.0.2\nPython interpreter will be restarted.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\nSystem.setProperty(\"ssl.ca.location\",\"dbfs:/FileStore/tables/cloudkarafka.ca\");"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f745f09-3558-47de-9e4f-fe5a339bb04e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">res1: String = dbfs:/FileStore/tables/cloudkarafka.ca\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res1: String = dbfs:/FileStore/tables/cloudkarafka.ca\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\nSystem.clearProperty(\"java.security.auth.login.config\");\nSystem.clearProperty(\"sasl.jaas.config\");\nSystem.setProperty(\"java.security.auth.login.config\", \"dbfs:/FileStore/tables/cloudkarafka.jaas\");\nSystem.setProperty(\"sasl.jaas.config\", \"dbfs:/FileStore/tables/cloudkarafka.jaas\");\nprintln(\"auth.lang:\" + System.getProperty(\"java.security.auth.login.config\"));\nprintln(\"sasl.jaas:\" + System.getProperty(\"sasl.jaas.config\"));\n\nspark.sparkContext.addFile(\"dbfs:/FileStore/tables/cloudkarafka.jaas\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd6f7258-387d-47b8-af6c-f5100c50789c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">auth.lang:dbfs:/FileStore/tables/cloudkarafka.jaas\nsasl.jaas:dbfs:/FileStore/tables/cloudkarafka.jaas\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">auth.lang:dbfs:/FileStore/tables/cloudkarafka.jaas\nsasl.jaas:dbfs:/FileStore/tables/cloudkarafka.jaas\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#https://docs.confluent.io/clients-confluent-kafka-python/current/index.html\nfrom confluent_kafka import Producer\n\nconf = {\n  'bootstrap.servers': \"omnibus-01.srvs.cloudkafka.com:9094,omnibus-03.srvs.cloudkafka.com:9094,omnibus-02.srvs.cloudkafka.com:9094\",\n  \"client.id\": \"Kafka-Producer\",\n  \"security.protocol\": \"SASL_SSL\",\n  \"sasl.mechanism\": \"SCRAM-SHA-256\",\n  \"sasl.username\": \"yrmfqh3q\",\n  \"sasl.password\": \"WfKJEYL_hwNUHVU8laSbj6gXkPIw_xuc\"\n}\nprint(\"conf:\", conf)\n\ntopic = \"yrmfqh3q-test\"\nfilePath = \"/FileStore/tables/customers.csv\"\nread = spark.read.format('csv').options(header='true').options(inferSchema='true').load(filePath)\nprint(type(read))\nread.show()\nprint(\"Read done...\", read)\n\n\n#for row in read.rdd.collect(): #doesnt scale on high volumes\n#  producer.produce(topic, key=\"message\", value=str(row.asDict()))\n#  producer.flush()\n#  print(\"Row Published\", row)\n\ndef publishToKafka(row):\n  print(\"publishToKafka:\", type(row), row)\n  \nread.foreach(publishToKafka)\n\nfrom kafka import KafkaProducer\n\nprint(type(read))\ndef publishToKafka2(rows):\n  print(\"publishToKafka2:\", type(rows), rows)\n  producer = KafkaProducer(\n    bootstrap_servers=['omnibus-01.srvs.cloudkafka.com:9094','omnibus-03.srvs.cloudkafka.com:9094','omnibus-02.srvs.cloudkafka.com:9094'],\n    client_id= \"Kafka-Producer\",\n    security_protocol=\"SASL_SSL\",\n    sasl_mechanism=\"SCRAM-SHA-256\",\n    sasl_plain_username=\"yrmfqh3q\",\n    sasl_plain_password=\"WfKJEYL_hwNUHVU8laSbj6gXkPIw_xuc\")\n  for row in rows:\n    print(\"publishToKafka2::row:\", type(row), row)\n    #producer.send(topic, {\"message\", str(row.asDict())} )\n    producer.send(topic, key=b'message', value=bytes(str(row.asDict()), \"utf-8\"))\n  producer.flush() #Flush after queuing up all the rows.\n\nread.foreachPartition(publishToKafka2)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0263d697-692b-4cce-a591-d68e9ae8df2d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">conf: {&#39;bootstrap.servers&#39;: &#39;omnibus-01.srvs.cloudkafka.com:9094,omnibus-03.srvs.cloudkafka.com:9094,omnibus-02.srvs.cloudkafka.com:9094&#39;, &#39;client.id&#39;: &#39;Kafka-Producer&#39;, &#39;security.protocol&#39;: &#39;SASL_SSL&#39;, &#39;sasl.mechanism&#39;: &#39;SCRAM-SHA-256&#39;, &#39;sasl.username&#39;: &#39;yrmfqh3q&#39;, &#39;sasl.password&#39;: &#39;WfKJEYL_hwNUHVU8laSbj6gXkPIw_xuc&#39;}\n&lt;class &#39;pyspark.sql.dataframe.DataFrame&#39;&gt;\n+---+-----+------+\n| id| name|active|\n+---+-----+------+\n|  1|Cust1|     1|\n|  2|Cust2|     0|\n|  3|Cust3|     1|\n+---+-----+------+\n\nRead done... DataFrame[id: int, name: string, active: int]\n&lt;class &#39;pyspark.sql.dataframe.DataFrame&#39;&gt;\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">conf: {&#39;bootstrap.servers&#39;: &#39;omnibus-01.srvs.cloudkafka.com:9094,omnibus-03.srvs.cloudkafka.com:9094,omnibus-02.srvs.cloudkafka.com:9094&#39;, &#39;client.id&#39;: &#39;Kafka-Producer&#39;, &#39;security.protocol&#39;: &#39;SASL_SSL&#39;, &#39;sasl.mechanism&#39;: &#39;SCRAM-SHA-256&#39;, &#39;sasl.username&#39;: &#39;yrmfqh3q&#39;, &#39;sasl.password&#39;: &#39;WfKJEYL_hwNUHVU8laSbj6gXkPIw_xuc&#39;}\n&lt;class &#39;pyspark.sql.dataframe.DataFrame&#39;&gt;\n+---+-----+------+\n id| name|active|\n+---+-----+------+\n  1|Cust1|     1|\n  2|Cust2|     0|\n  3|Cust3|     1|\n+---+-----+------+\n\nRead done... DataFrame[id: int, name: string, active: int]\n&lt;class &#39;pyspark.sql.dataframe.DataFrame&#39;&gt;\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2269f05d-fe4c-4a6c-a411-1211627a8d31"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"kafka-producer","dashboards":[],"language":"python","widgets":{},"notebookOrigID":1275481656822739}},"nbformat":4,"nbformat_minor":0}
