{"cells":[{"cell_type":"code","source":["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sun Aug  9 11:21:19 2020\n\n@author: sunilmiriyala\n\"\"\"\n#pyspark local run\n\"\"\"\nhttps://spark.apache.org/docs/latest/api/python/pyspark.sql.html\nhttps://spark.apache.org/docs/latest/api/python/index.html\n\nRun:\nsource /opt/codebase/PYTHON3/bin/activate \ncd <your-path>/PySpark/\nsh ./setenv.sh\necho $SPARK_HOME\ncd src/python/\n$SPARK_HOME/bin/spark-submit pyspark-local.py\n\"\"\"\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark import SparkContext, SparkConf\nfrom jproperties import Properties\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\nfrom pyspark.sql.functions import col\n\nclass PySparkLocal:\n    def __init__(self, **kwargs):\n        print(\"__init__::kwargs:%s\" % kwargs)\n        self.appname = kwargs.get(\"name\", kwargs.get(\"spark.name\", \"PySpark-Local\"))\n        self.conf = SparkConf().setAppName(self.appname).setMaster(kwargs.get(\"master\", kwargs.get(\"spark.master\", \"local\")))\n        print(\"__init__::self.conf:%s\" % self.conf)\n        #self.sc = SparkContext(conf=self.conf)\n        #print(\"__init__::self.sc:%s\" % self.sc)\n        self.spark = SparkSession.builder.appName(self.appname).getOrCreate()\n        print(\"__init__::self.spark:%s\" % self.spark)\n        self.spark.sparkContext.setLogLevel('WARN')\n        #Local spark env (choices: local, cluster,..)\n        self.file_location = kwargs.get(\"data.source.location\")\n        self.file_names = kwargs.get(\"data.source.files\")\n    \n    def mytransform(self, r):\n        print(\"transform::r:\", r)\n\n    def loadFiles(self):\n        print(\"loadFiles\")\n        files = self.file_names.split(\",\")\n        print(\"loadFiles::files:\", files, \", path:\", self.file_location)\n        all_cust_rdd = self.spark.sparkContext.textFile(self.file_location + files[0])\n        print(\"loadFiles::all_cust_rdd (RDD):%s\" % all_cust_rdd)\n        all_cust_rdd.foreach(lambda r: print(\"Row:\", r))\n        all_cust_fmap_rdd = all_cust_rdd.flatMap(lambda r: r.split(\",\"))\n        print(\"loadFiles::all_cust_fmap_rdd (RDD):%s\" % all_cust_fmap_rdd)\n        all_cust_fmap_rdd.foreach(lambda c: print(\"Cell:\", c))\n\n        all_cust_df = self.readFile(filePath=self.file_location + files[0])\n        active_cust_df = all_cust_df.filter(all_cust_df.active == 1)\n        \n        all_apps_df = self.readFile(filePath=self.file_location + files[1])\n        active_apps_df = all_apps_df.filter(all_apps_df.active == 1)\n\n        all_cust_apps_df = self.readFile(filePath=self.file_location + files[2])\n        active_cust_apps_df = all_cust_apps_df.filter(all_cust_apps_df.active == 1)\n\n        cust_custapps_join_df = active_cust_df.join(other=active_cust_apps_df, on=(active_cust_df.id == active_cust_apps_df.cust_id), how='inner')\n        print(\"loadFiles::cust_custapps_join_df:%s\" % (cust_custapps_join_df))\n        cust_custapps_join_df.printSchema()\n        cust_custapps_join_df.show()\n\n        all_apps_df.createOrReplaceTempView(\"ALL_APPS\")\n        all_cust_apps_df.createOrReplaceTempView(\"ALL_CUST_APPS\")\n        apps_custapps_join_df = self.spark.sql(\"select aa.name as app_name, aa.id as app_id, aca.cust_id as cust_id, aca.active as cust_app_active from ALL_APPS aa, ALL_CUST_APPS aca where aa.id == aca.app_id and aa.active == 1\")\n        print(\"loadFiles::apps_custapps_join_df:%s\" % (apps_custapps_join_df))\n        apps_custapps_join_df.printSchema()\n        apps_custapps_join_df.show()\n        print(\"loadFiles::groupBy::name:\")\n        apps_custapps_join_df.groupBy(\"app_name\").count().show()\n        \n        apps_custapps_join_df.write.partitionBy(\"app_name\").format(\"avro\").mode(\"overwrite\").save(self.file_location + \"active_apps_custapps.avro\")\n        print(\"loadFiles::Successfully written to avro file\")\n        avro_oms_df = self.spark.read.format(\"avro\").load(self.file_location + \"active_apps_custapps.avro\").where(col(\"app_name\") == \"OMS\").show()\n        print(\"loadFiles::avro_oms_df::\", avro_oms_df)\n\n\n    def readFile(self, filePath):\n        print(\"readFile::**********************\")\n        print(\"readFile::filePath:%s\" % filePath)\n        df = self.spark.read.format('csv').options(header='true').options(inferSchema='true').load(filePath).cache()\n        print(\"readFile::df:%s\" % (df))\n        df.printSchema()\n        df.show()\n        print(\"readFile::df.count:%s\" % df.count())\n        return df\n\nif __name__ == '__main__':\n    #More params\n    #master = None, appName = None, sparkHome = None, pyFiles = None, \n    #environment = None, batchSize = 0, serializer = PickleSerializer(), \n    #conf = None, gateway = None, jsc = None, profiler_cls = <class 'pyspark.profiler.BasicProfiler'>\n    \"\"\"\n        \"spark.master\":\"local\",\n        \"spark.name\":\"MySpark-CSV\",\n        \"data.source.format\":\"csv\",\n        \"data.source.location\":\"s3://pyspark-sunil/data/\",\n        \"data.source.files\":\"customers.csv,applications.csv,customer-applications.csv\"\n    \"\"\"\n    import json\n    import argparse\n\n    #parser = argparse.ArgumentParser()\n    #parser.add_argument(\"--p\", help=\"Properties File\", dest='prop', required=True)\n    #args = parser.parse_args()\n    kw = {\"spark.master\": \"local\", \"spark.name\": \"MySpark-CSV\", \"data.source.type\": \"file\", \"data.source.format\": \"csv\", \"data.source.location\": \"/FileStore/tables/\", \"data.source.files\":\"customers.csv,applications.csv,customer_applications.csv\"}\n    #configs = Properties()\n    #with open(\"/FileStore/tables/pyspark_csv_properties.csv\", 'rb') as config_file:\n    #    configs.load(config_file)\n    #print(\"configs:\", configs)\n    #for p in configs:\n    #print(\"P:Name:%s, Val:%s\" % (p, configs.get(p).data))\n    #kw[p] = configs.get(p).data\n    print(\"configs::%s, kw:%s\" % (configs, kw))\n    psl = PySparkLocal(**kw)\n    psl.loadFiles()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60cd683f-6db4-48c0-86a4-43ffba6ac28b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">configs::&lt;jproperties.Properties object at 0x7f8b35433d90&gt;, kw:{&#39;spark.master&#39;: &#39;local&#39;, &#39;spark.name&#39;: &#39;MySpark-CSV&#39;, &#39;data.source.type&#39;: &#39;file&#39;, &#39;data.source.format&#39;: &#39;csv&#39;, &#39;data.source.location&#39;: &#39;/FileStore/tables/&#39;, &#39;data.source.files&#39;: &#39;customers.csv,applications.csv,customer_applications.csv&#39;}\n__init__::kwargs:{&#39;spark.master&#39;: &#39;local&#39;, &#39;spark.name&#39;: &#39;MySpark-CSV&#39;, &#39;data.source.type&#39;: &#39;file&#39;, &#39;data.source.format&#39;: &#39;csv&#39;, &#39;data.source.location&#39;: &#39;/FileStore/tables/&#39;, &#39;data.source.files&#39;: &#39;customers.csv,applications.csv,customer_applications.csv&#39;}\n__init__::self.conf:&lt;pyspark.conf.SparkConf object at 0x7f8b3f644290&gt;\n__init__::self.spark:&lt;pyspark.sql.session.SparkSession object at 0x7f8b3d778890&gt;\nloadFiles\nloadFiles::files: [&#39;customers.csv&#39;, &#39;applications.csv&#39;, &#39;customer_applications.csv&#39;] , path: /FileStore/tables/\nloadFiles::all_cust_rdd (RDD):/FileStore/tables/customers.csv MapPartitionsRDD[150] at textFile at NativeMethodAccessorImpl.java:0\nloadFiles::all_cust_fmap_rdd (RDD):PythonRDD[152] at RDD at PythonRDD.scala:58\nreadFile::**********************\nreadFile::filePath:/FileStore/tables/customers.csv\nreadFile::df:DataFrame[id: int, name: string, active: int]\nroot\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- active: integer (nullable = true)\n\n+---+-----+------+\n| id| name|active|\n+---+-----+------+\n|  1|Cust1|     1|\n|  2|Cust2|     0|\n|  3|Cust3|     1|\n+---+-----+------+\n\nreadFile::df.count:3\nreadFile::**********************\nreadFile::filePath:/FileStore/tables/applications.csv\nreadFile::df:DataFrame[id: int, name: string, active: int]\nroot\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- active: integer (nullable = true)\n\n+---+----+------+\n| id|name|active|\n+---+----+------+\n|  1| OMS|     1|\n|  2| TMS|     1|\n+---+----+------+\n\nreadFile::df.count:2\nreadFile::**********************\nreadFile::filePath:/FileStore/tables/customer_applications.csv\nreadFile::df:DataFrame[id: int, cust_id: int, app_id: int, active: int]\nroot\n |-- id: integer (nullable = true)\n |-- cust_id: integer (nullable = true)\n |-- app_id: integer (nullable = true)\n |-- active: integer (nullable = true)\n\n+---+-------+------+------+\n| id|cust_id|app_id|active|\n+---+-------+------+------+\n|  1|      1|     1|     1|\n|  2|      1|     2|     1|\n|  2|      2|     1|     1|\n|  2|      2|     2|     1|\n|  3|      3|     1|     1|\n|  4|      3|     2|     0|\n+---+-------+------+------+\n\nreadFile::df.count:6\nloadFiles::cust_custapps_join_df:DataFrame[id: int, name: string, active: int, id: int, cust_id: int, app_id: int, active: int]\nroot\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- active: integer (nullable = true)\n |-- id: integer (nullable = true)\n |-- cust_id: integer (nullable = true)\n |-- app_id: integer (nullable = true)\n |-- active: integer (nullable = true)\n\n+---+-----+------+---+-------+------+------+\n| id| name|active| id|cust_id|app_id|active|\n+---+-----+------+---+-------+------+------+\n|  1|Cust1|     1|  1|      1|     1|     1|\n|  1|Cust1|     1|  2|      1|     2|     1|\n|  3|Cust3|     1|  3|      3|     1|     1|\n+---+-----+------+---+-------+------+------+\n\nloadFiles::apps_custapps_join_df:DataFrame[app_name: string, app_id: int, cust_id: int, cust_app_active: int]\nroot\n |-- app_name: string (nullable = true)\n |-- app_id: integer (nullable = true)\n |-- cust_id: integer (nullable = true)\n |-- cust_app_active: integer (nullable = true)\n\n+--------+------+-------+---------------+\n|app_name|app_id|cust_id|cust_app_active|\n+--------+------+-------+---------------+\n|     OMS|     1|      1|              1|\n|     TMS|     2|      1|              1|\n|     OMS|     1|      2|              1|\n|     TMS|     2|      2|              1|\n|     OMS|     1|      3|              1|\n|     TMS|     2|      3|              0|\n+--------+------+-------+---------------+\n\nloadFiles::groupBy::name:\n+--------+-----+\n|app_name|count|\n+--------+-----+\n|     TMS|    3|\n|     OMS|    3|\n+--------+-----+\n\nloadFiles::Successfully written to avro file\n+------+-------+---------------+--------+\n|app_id|cust_id|cust_app_active|app_name|\n+------+-------+---------------+--------+\n|     1|      1|              1|     OMS|\n|     1|      2|              1|     OMS|\n|     1|      3|              1|     OMS|\n+------+-------+---------------+--------+\n\nloadFiles::avro_oms_df:: None\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">configs::&lt;jproperties.Properties object at 0x7f8b35433d90&gt;, kw:{&#39;spark.master&#39;: &#39;local&#39;, &#39;spark.name&#39;: &#39;MySpark-CSV&#39;, &#39;data.source.type&#39;: &#39;file&#39;, &#39;data.source.format&#39;: &#39;csv&#39;, &#39;data.source.location&#39;: &#39;/FileStore/tables/&#39;, &#39;data.source.files&#39;: &#39;customers.csv,applications.csv,customer_applications.csv&#39;}\n__init__::kwargs:{&#39;spark.master&#39;: &#39;local&#39;, &#39;spark.name&#39;: &#39;MySpark-CSV&#39;, &#39;data.source.type&#39;: &#39;file&#39;, &#39;data.source.format&#39;: &#39;csv&#39;, &#39;data.source.location&#39;: &#39;/FileStore/tables/&#39;, &#39;data.source.files&#39;: &#39;customers.csv,applications.csv,customer_applications.csv&#39;}\n__init__::self.conf:&lt;pyspark.conf.SparkConf object at 0x7f8b3f644290&gt;\n__init__::self.spark:&lt;pyspark.sql.session.SparkSession object at 0x7f8b3d778890&gt;\nloadFiles\nloadFiles::files: [&#39;customers.csv&#39;, &#39;applications.csv&#39;, &#39;customer_applications.csv&#39;] , path: /FileStore/tables/\nloadFiles::all_cust_rdd (RDD):/FileStore/tables/customers.csv MapPartitionsRDD[150] at textFile at NativeMethodAccessorImpl.java:0\nloadFiles::all_cust_fmap_rdd (RDD):PythonRDD[152] at RDD at PythonRDD.scala:58\nreadFile::**********************\nreadFile::filePath:/FileStore/tables/customers.csv\nreadFile::df:DataFrame[id: int, name: string, active: int]\nroot\n-- id: integer (nullable = true)\n-- name: string (nullable = true)\n-- active: integer (nullable = true)\n\n+---+-----+------+\n id| name|active|\n+---+-----+------+\n  1|Cust1|     1|\n  2|Cust2|     0|\n  3|Cust3|     1|\n+---+-----+------+\n\nreadFile::df.count:3\nreadFile::**********************\nreadFile::filePath:/FileStore/tables/applications.csv\nreadFile::df:DataFrame[id: int, name: string, active: int]\nroot\n-- id: integer (nullable = true)\n-- name: string (nullable = true)\n-- active: integer (nullable = true)\n\n+---+----+------+\n id|name|active|\n+---+----+------+\n  1| OMS|     1|\n  2| TMS|     1|\n+---+----+------+\n\nreadFile::df.count:2\nreadFile::**********************\nreadFile::filePath:/FileStore/tables/customer_applications.csv\nreadFile::df:DataFrame[id: int, cust_id: int, app_id: int, active: int]\nroot\n-- id: integer (nullable = true)\n-- cust_id: integer (nullable = true)\n-- app_id: integer (nullable = true)\n-- active: integer (nullable = true)\n\n+---+-------+------+------+\n id|cust_id|app_id|active|\n+---+-------+------+------+\n  1|      1|     1|     1|\n  2|      1|     2|     1|\n  2|      2|     1|     1|\n  2|      2|     2|     1|\n  3|      3|     1|     1|\n  4|      3|     2|     0|\n+---+-------+------+------+\n\nreadFile::df.count:6\nloadFiles::cust_custapps_join_df:DataFrame[id: int, name: string, active: int, id: int, cust_id: int, app_id: int, active: int]\nroot\n-- id: integer (nullable = true)\n-- name: string (nullable = true)\n-- active: integer (nullable = true)\n-- id: integer (nullable = true)\n-- cust_id: integer (nullable = true)\n-- app_id: integer (nullable = true)\n-- active: integer (nullable = true)\n\n+---+-----+------+---+-------+------+------+\n id| name|active| id|cust_id|app_id|active|\n+---+-----+------+---+-------+------+------+\n  1|Cust1|     1|  1|      1|     1|     1|\n  1|Cust1|     1|  2|      1|     2|     1|\n  3|Cust3|     1|  3|      3|     1|     1|\n+---+-----+------+---+-------+------+------+\n\nloadFiles::apps_custapps_join_df:DataFrame[app_name: string, app_id: int, cust_id: int, cust_app_active: int]\nroot\n-- app_name: string (nullable = true)\n-- app_id: integer (nullable = true)\n-- cust_id: integer (nullable = true)\n-- cust_app_active: integer (nullable = true)\n\n+--------+------+-------+---------------+\napp_name|app_id|cust_id|cust_app_active|\n+--------+------+-------+---------------+\n     OMS|     1|      1|              1|\n     TMS|     2|      1|              1|\n     OMS|     1|      2|              1|\n     TMS|     2|      2|              1|\n     OMS|     1|      3|              1|\n     TMS|     2|      3|              0|\n+--------+------+-------+---------------+\n\nloadFiles::groupBy::name:\n+--------+-----+\napp_name|count|\n+--------+-----+\n     TMS|    3|\n     OMS|    3|\n+--------+-----+\n\nloadFiles::Successfully written to avro file\n+------+-------+---------------+--------+\napp_id|cust_id|cust_app_active|app_name|\n+------+-------+---------------+--------+\n     1|      1|              1|     OMS|\n     1|      2|              1|     OMS|\n     1|      3|              1|     OMS|\n+------+-------+---------------+--------+\n\nloadFiles::avro_oms_df:: None\n</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-csv","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4430552160603186}},"nbformat":4,"nbformat_minor":0}
