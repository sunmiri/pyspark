{"cells":[{"cell_type":"code","source":["pip install json5"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27ec5b96-7fcd-4be3-bc66-9bdf4d002925"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting json5\n  Downloading json5-0.9.5-py2.py3-none-any.whl (17 kB)\nInstalling collected packages: json5\nSuccessfully installed json5-0.9.5\nPython interpreter will be restarted.\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting json5\n  Downloading json5-0.9.5-py2.py3-none-any.whl (17 kB)\nInstalling collected packages: json5\nSuccessfully installed json5-0.9.5\nPython interpreter will be restarted.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#Use case\n#Community Master Data: Users, Roles, UserRole\n#Transaction Data: Transactions (transId, userId, some details)\n#From userId, get the relevant username and perform anyother lookups\n\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col,sum,avg\nfrom pyspark.sql.types import Row\nimport json5 as json\n\ncolumns = [\"RId\",\"RName\", \"RActive\"]\ndata = [(1, \"Role1\", 1),\n    (2, \"Role2\", 1),\n    (3, \"Role3\", 1)]\ndf_role = spark.createDataFrame(data=data, schema=columns)\ndf_role.createOrReplaceTempView(\"roles\")\nbc_role = spark.sparkContext.broadcast(df_role.toJSON().collect())\n\ncolumns = [\"UId\",\"UName\", \"UActive\"]\ndata = [(1, \"User1\", 1),\n    (2, \"User2\", 1),\n    (3, \"User3\", 0),\n    (4, \"User4\", 1)]\ndf_user = spark.createDataFrame(data=data, schema=columns)\ndf_user.createOrReplaceTempView(\"users\")\nbc_user = spark.sparkContext.broadcast(df_user.toJSON().collect())\n\ncolumns = [\"UId\", \"RId\"]\ndata = [(1, 1), (1, 2), (2, 3), (3, 1), (3,2), (3,3), (4, 1), (4,3)]\ndf_user_role = spark.createDataFrame(data=data, schema=columns)\ndf_user_role.createOrReplaceTempView(\"user_role\")\nbc_user_role = spark.sparkContext.broadcast(df_user_role.toJSON().collect())\n\n\ndef myMapFunction(rec, bc_user, bc_role, bc_user_role):\n  print(\"myMapFunction::Type:%s, Data:%s\" % (type(rec), rec))\n  #myMapFunction::Type:<class 'pyspark.sql.types.Row'>, Data:Row(TId=4, UId=3, TDesc='TDesc4', TDate='2021-04-16T17:14:00')\n  row_dict = rec.asDict()\n  userId = row_dict.get(\"UId\")\n  print(\"myMapFunction::user:%s, bc_user:%s, bc_user.value:%s\" % (userId, type(bc_user), type(bc_user.value)))\n  enrich1_val = \"\"\n  for v in bc_user.value:\n    print(\"myMapFunction::type(v):%s, v:%s\" % (type(v), v))\n    #myMapFunction::type(v):<class 'str'>, v:{\"UId\":1,\"UName\":\"U1\",\"UActive\":1}\n    if type(v) == str:\n      v = json.loads(v)\n    if v.get(\"UId\") == userId:\n      enrich1_val = v.get(\"UName\")\n  #myMapFunction::user:3, bc_user:<class 'pyspark.broadcast.Broadcast'>, bc_user.value:<class 'list'>\n  row_dict[\"ENRICH_1\"] = enrich1_val #Adding a new column and setting some value.\n  print(\"myMapFunction::row_dict:%s\" % row_dict)\n  #myMapFunction::row_dict:{'TId': 4, 'UId': 3, 'TDesc': 'TDesc4', 'TDate': '2021-04-16T17:14:00', 'ENRICH_1': 'constant_value'}\n  return Row(**row_dict)\n\ncolumns = [\"TId\", \"UId\", \"TDesc\", \"TDate\"]\ndata = [\n  (1, 1, \"TDesc1\", \"2021-04-16T17:11:00\"), \n  (2, 1, \"TDesc2\", \"2021-04-16T17:12:00\"), \n  (3, 4, \"TDesc3\", \"2021-04-16T17:13:00\"), \n  (4, 3, \"TDesc4\", \"2021-04-16T17:14:00\"), \n  (5, 2, \"TDesc5\", \"2021-04-16T17:15:00\")]\ndf_trans = spark.createDataFrame(data=data, schema=columns)\n\ndf_new_rdd = df_trans.rdd.map(lambda r: myMapFunction(r, bc_user, bc_role, bc_user_role))\nprint(\"df_new_rdd:\", df_new_rdd)\ndf_new_rdd.toDF([\"TId\", \"UId\", \"TDesc\", \"TDate\", \"ENRICH_1\"]).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e9ba1dd9-41cc-41c1-bfba-e9f6a6c09423"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import StructType,StructField, StringType, LongType\n\ndef myFlatMapFunction(rec):\n  print(\"myFlatMapFunction::Type:%s, Data:%s\" % (type(rec), rec))\n  #myFlatMapFunction::Type:<class 'pyspark.sql.types.Row'>, Data:Row(TId=4, UId=3, TDesc='TDesc4', TDate='2021-04-16T17:14:00')\n  return rec\n\nprint(\"Source Dataframe::df_trans:\", df_trans)\ndf_trans.printSchema()\ndf_trans.show()\ndf_new_rdd = df_trans.rdd.flatMap(lambda r: myFlatMapFunction(r))\nprint(\"df_new_rdd:\", df_new_rdd)\n#df_new_rdd.toDF() #Can not infer schema for type: <class 'int'>\n\ndef myForEach(rec):\n  print(\"myForEach::type:%s, rec:%s\" % (type(rec), rec))\n  #myForEach::type:<class 'int'>, rec:4\n\n#Foreach\ndf_new_rdd.foreach(lambda r: myForEach(r))\n\nmyschema = StructType([       \n    StructField('TId', LongType(), True),\n    StructField('UId', LongType(), True),\n    StructField('TDesc', StringType(), True),\n    StructField('TDate', StringType(), True)\n])\ndf_new_rdd_df = spark.createDataFrame(df_new_rdd, schema = myschema)\nprint(\"RDD to DF::df_new_rdd_df:\", df_new_rdd_df)\ndf_new_rdd_df.printSchema()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"836bad72-c31d-4a38-a55d-60de0880206a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import json5 as json\nimport pyspark.sql.functions as f\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col,sum,avg\nfrom pyspark.sql.types import Row\nimport json5 as json\n\ncolumns = [\"Id\", \"Name\", \"City\", \"State\", \"Country\", \"Active\"]\ndata = [\n  (1, \"Store-A\", \"New York\", \"New York\", \"USA\", 1),\n  (2, \"Store-B\", \"Washington\", \"Seattle\", \"USA\", 1),\n  (3, \"Store-C\", \"Dallas\", \"Texas\", \"USA\", 1)\n]\ndf_store = spark.createDataFrame(data= data, schema = columns )\nprint(\"df_store\")\ndf_store.show()\nstores = []\n#['{\"Id\":1,\"Name\":\"Store-A\",\"City\":\"New York\",\"State\":\"New York\",\"Country\":\"USA\",\"Active\":1}']\nfor e in df_store.toJSON().collect():\n  e = json.loads(e)\n  stores.append(e)\nprint(\"Stores as JSON Array\", stores)\nbc_stores = spark.sparkContext.broadcast(stores)\n\ncolumns = [\"Id\", \"Name\", \"Desc\", \"Active\"]\ndata = [(1, \"Bananas\", \"Bananas\", \"1\"),\n             (2,\"Apples\",\"Gala Apples\",\"1\"),\n             (3, \"Organes\", \"Naval Oranges\", \"1\")]\n\ndf_items = spark.createDataFrame(data= data, schema = columns )\nprint(\"df_items\")\ndf_items.show()\n#df_items.toJSON().collect(): ['{\"Id\":1,\"Name\":\"Bananas\",\"Desc\":\"Bananas\",\"Active\":\"1\"}']\nitems = []\nfor e in df_items.toJSON().collect():\n  e = json.loads(e)\n  items.append(e)\nprint(\"Items as JSON Array\", items)\nbc_items = spark.sparkContext.broadcast(items)\n\n\ndef myTransEnrich(row, bc_stores, bc_items):\n  print(\"myTransEnrich::type:%s, data:%s\" % (type(row), row))\n  #myTransEnrich::type:<class 'pyspark.sql.types.Row'>, data:Row(TId=1, StoreId=3, ItemId=3, TDesc='Oranges for StoreC@DAL', TDate='2021-04-19T02:01:01')\n  row_dict = row.asDict()\n  print(\"myTransEnrich::row_dict:\", type(row_dict), row_dict)\n  #myTransEnrich::row_dict: <class 'dict'> {'TId': 1, 'StoreId': 2, 'ItemId': 1, 'TDesc': 'Bananas for StoreA@NYC', 'TDate': '2021-04-19T03:01:01'}\n  itemId = row_dict.get(\"ItemId\")\n  storeId = row_dict.get(\"StoreId\")\n        \n  #print(\"myTransEnrich::bc_user.value::\", type(bc_stores.value), bc_stores.value)\n  #myTransEnrich::bc_user.value:: <class 'list'> [{'Id': 1, 'Name': 'Store-A', 'City': 'New York', 'State': 'New York', 'Country': 'USA', 'Active': 1}]\n  myitem = list(filter(lambda x:x[\"Id\"]==storeId, bc_stores.value))\n  print(\"myTransEnrich::myitem:\", (myitem))\n  #myTransEnrich::myitem: [{'Id': 3, 'Name': 'Store-C', 'City': 'Dallas', 'State': 'Texas', 'Country': 'USA', 'Active': 1}]\n  myitem = myitem[0]\n  row_dict[\"ItemName\"] = myitem[\"Name\"]\n\n  #print(\"myTransEnrich::bc_items.value::\", type(bc_items.value), bc_items.value)\n  #myTransEnrich::bc_items.value:: <class 'list'> [{'Id': 1, 'Name': 'Bananas', 'Desc': 'Bananas', 'Active': '1'}]\n  mystore = list(filter(lambda x:x[\"Id\"]==itemId, bc_items.value))\n  print(\"myTransEnrich::mystore:\", (mystore))\n  #myTransEnrich::mystore: [{'Id': 3, 'Name': 'Organes', 'Desc': 'Naval Oranges', 'Active': '1'}]\n  mystore = mystore[0]\n  row_dict[\"StoreName\"] = mystore[\"Name\"]\n  \n  return Row(**row_dict)\n  \ncolumns = [\"TId\", \"StoreId\", \"ItemId\", \"TDesc\", \"TDate\"]\ndata = [(1, 1, 2, \"Apples for StoreB@WAS\", \"2021-04-19T01:01:01\"),\n             (1, 3, 3, \"Oranges for StoreC@DAL\", \"2021-04-19T02:01:01\"),\n             (1, 2, 1, \"Bananas for StoreA@NYC\", \"2021-04-19T03:01:01\")]\ndf_trans = spark.createDataFrame(data= data, schema = columns )\nprint(\"df_trans\")\ndf_trans.show()\n\n#Using Map\ndf_enriched = df_trans.rdd.map(lambda row: myTransEnrich(row, bc_stores, bc_items))\nprint(\"df_enriched:0:\", df_enriched)\ndf_enriched.toDF([\"TId\", \"StoreId\", \"ItemId\", \"TDesc\", \"TDate\"]).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"540f2242-0d17-4afd-baff-b7b09c79d79e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">df_store\n+---+-------+----------+--------+-------+------+\n| Id|   Name|      City|   State|Country|Active|\n+---+-------+----------+--------+-------+------+\n|  1|Store-A|  New York|New York|    USA|     1|\n|  2|Store-B|Washington| Seattle|    USA|     1|\n|  3|Store-C|    Dallas|   Texas|    USA|     1|\n+---+-------+----------+--------+-------+------+\n\nStores as JSON Array [{&#39;Id&#39;: 1, &#39;Name&#39;: &#39;Store-A&#39;, &#39;City&#39;: &#39;New York&#39;, &#39;State&#39;: &#39;New York&#39;, &#39;Country&#39;: &#39;USA&#39;, &#39;Active&#39;: 1}, {&#39;Id&#39;: 2, &#39;Name&#39;: &#39;Store-B&#39;, &#39;City&#39;: &#39;Washington&#39;, &#39;State&#39;: &#39;Seattle&#39;, &#39;Country&#39;: &#39;USA&#39;, &#39;Active&#39;: 1}, {&#39;Id&#39;: 3, &#39;Name&#39;: &#39;Store-C&#39;, &#39;City&#39;: &#39;Dallas&#39;, &#39;State&#39;: &#39;Texas&#39;, &#39;Country&#39;: &#39;USA&#39;, &#39;Active&#39;: 1}]\ndf_items\n+---+-------+-------------+------+\n| Id|   Name|         Desc|Active|\n+---+-------+-------------+------+\n|  1|Bananas|      Bananas|     1|\n|  2| Apples|  Gala Apples|     1|\n|  3|Organes|Naval Oranges|     1|\n+---+-------+-------------+------+\n\nItems as JSON Array [{&#39;Id&#39;: 1, &#39;Name&#39;: &#39;Bananas&#39;, &#39;Desc&#39;: &#39;Bananas&#39;, &#39;Active&#39;: &#39;1&#39;}, {&#39;Id&#39;: 2, &#39;Name&#39;: &#39;Apples&#39;, &#39;Desc&#39;: &#39;Gala Apples&#39;, &#39;Active&#39;: &#39;1&#39;}, {&#39;Id&#39;: 3, &#39;Name&#39;: &#39;Organes&#39;, &#39;Desc&#39;: &#39;Naval Oranges&#39;, &#39;Active&#39;: &#39;1&#39;}]\ndf_trans\n+---+-------+------+--------------------+-------------------+\n|TId|StoreId|ItemId|               TDesc|              TDate|\n+---+-------+------+--------------------+-------------------+\n|  1|      1|     2|Apples for StoreB...|2021-04-19T01:01:01|\n|  1|      3|     3|Oranges for Store...|2021-04-19T02:01:01|\n|  1|      2|     1|Bananas for Store...|2021-04-19T03:01:01|\n+---+-------+------+--------------------+-------------------+\n\ndf_enriched:0: PythonRDD[853] at RDD at PythonRDD.scala:58\n+---+-------+------+--------------------+-------------------+--------+---------+\n|TId|StoreId|ItemId|               TDesc|              TDate|ItemName|StoreName|\n+---+-------+------+--------------------+-------------------+--------+---------+\n|  1|      1|     2|Apples for StoreB...|2021-04-19T01:01:01| Store-A|   Apples|\n|  1|      3|     3|Oranges for Store...|2021-04-19T02:01:01| Store-C|  Organes|\n|  1|      2|     1|Bananas for Store...|2021-04-19T03:01:01| Store-B|  Bananas|\n+---+-------+------+--------------------+-------------------+--------+---------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">df_store\n+---+-------+----------+--------+-------+------+\n Id|   Name|      City|   State|Country|Active|\n+---+-------+----------+--------+-------+------+\n  1|Store-A|  New York|New York|    USA|     1|\n  2|Store-B|Washington| Seattle|    USA|     1|\n  3|Store-C|    Dallas|   Texas|    USA|     1|\n+---+-------+----------+--------+-------+------+\n\nStores as JSON Array [{&#39;Id&#39;: 1, &#39;Name&#39;: &#39;Store-A&#39;, &#39;City&#39;: &#39;New York&#39;, &#39;State&#39;: &#39;New York&#39;, &#39;Country&#39;: &#39;USA&#39;, &#39;Active&#39;: 1}, {&#39;Id&#39;: 2, &#39;Name&#39;: &#39;Store-B&#39;, &#39;City&#39;: &#39;Washington&#39;, &#39;State&#39;: &#39;Seattle&#39;, &#39;Country&#39;: &#39;USA&#39;, &#39;Active&#39;: 1}, {&#39;Id&#39;: 3, &#39;Name&#39;: &#39;Store-C&#39;, &#39;City&#39;: &#39;Dallas&#39;, &#39;State&#39;: &#39;Texas&#39;, &#39;Country&#39;: &#39;USA&#39;, &#39;Active&#39;: 1}]\ndf_items\n+---+-------+-------------+------+\n Id|   Name|         Desc|Active|\n+---+-------+-------------+------+\n  1|Bananas|      Bananas|     1|\n  2| Apples|  Gala Apples|     1|\n  3|Organes|Naval Oranges|     1|\n+---+-------+-------------+------+\n\nItems as JSON Array [{&#39;Id&#39;: 1, &#39;Name&#39;: &#39;Bananas&#39;, &#39;Desc&#39;: &#39;Bananas&#39;, &#39;Active&#39;: &#39;1&#39;}, {&#39;Id&#39;: 2, &#39;Name&#39;: &#39;Apples&#39;, &#39;Desc&#39;: &#39;Gala Apples&#39;, &#39;Active&#39;: &#39;1&#39;}, {&#39;Id&#39;: 3, &#39;Name&#39;: &#39;Organes&#39;, &#39;Desc&#39;: &#39;Naval Oranges&#39;, &#39;Active&#39;: &#39;1&#39;}]\ndf_trans\n+---+-------+------+--------------------+-------------------+\nTId|StoreId|ItemId|               TDesc|              TDate|\n+---+-------+------+--------------------+-------------------+\n  1|      1|     2|Apples for StoreB...|2021-04-19T01:01:01|\n  1|      3|     3|Oranges for Store...|2021-04-19T02:01:01|\n  1|      2|     1|Bananas for Store...|2021-04-19T03:01:01|\n+---+-------+------+--------------------+-------------------+\n\ndf_enriched:0: PythonRDD[853] at RDD at PythonRDD.scala:58\n+---+-------+------+--------------------+-------------------+--------+---------+\nTId|StoreId|ItemId|               TDesc|              TDate|ItemName|StoreName|\n+---+-------+------+--------------------+-------------------+--------+---------+\n  1|      1|     2|Apples for StoreB...|2021-04-19T01:01:01| Store-A|   Apples|\n  1|      3|     3|Oranges for Store...|2021-04-19T02:01:01| Store-C|  Organes|\n  1|      2|     1|Bananas for Store...|2021-04-19T03:01:01| Store-B|  Bananas|\n+---+-------+------+--------------------+-------------------+--------+---------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark.sql.functions as f\nfrom pyspark.sql.types import StructType,StructField, StringType, LongType\n\nprint(\"================== DataFrame.JOIN ==================\")\n\ncolumns = [\"Id\", \"Name\", \"City\", \"State\", \"Country\", \"Active\"]\ndata = [\n  (1, \"Store-A\", \"New York\", \"New York\", \"USA\", 1),\n  (2, \"Store-B\", \"Washington\", \"Seattle\", \"USA\", 1),\n  (3, \"Store-C\", \"Dallas\", \"Texas\", \"USA\", 1)\n]\ndf_store = spark.createDataFrame(data= data, schema = columns )\n\ncolumns = [\"Id\", \"Name\", \"Desc\", \"Active\"]\ndata = [(1, \"Bananas\", \"Bananas\", \"1\"),\n             (2,\"Apples\",\"Gala Apples\",\"1\"),\n             (3, \"Organes\", \"Naval Oranges\", \"1\")]\n\ndf_items = spark.createDataFrame(data= data, schema = columns )\n\ncolumns = [\"TId\", \"StoreId\", \"ItemId\", \"TDesc\", \"TDate\"]\ndata = [(1, 1, 2, \"Apples for StoreB@WAS\", \"2021-04-19T01:01:01\"),\n             (1, 3, 3, \"Oranges for StoreC@DAL\", \"2021-04-19T02:01:01\"),\n             (1, 2, 1, \"Bananas for StoreA@NYC\", \"2021-04-19T03:01:01\")]\ndf_trans = spark.createDataFrame(data= data, schema = columns )\n\n#using dataframe joins\ncolumns = [\"TId\", \"TDesc\", \"TDate\", \"Name\", \"StoreId\"]\n#Trans: [\"TId\", \"StoreId\", \"ItemId\", \"TDesc\", \"TDate\"]\n#Items: [\"Id\", \"Name\", \"Desc\", \"Active\"]\ndf_trans_items = df_trans.join(df_items, df_items.Id == df_trans.ItemId, \"inner\").select([col for col in columns])\ndf_trans_items = df_trans_items.withColumnRenamed(\"Name\", \"ItemName\")\nprint(\"df_trans_items:\", df_trans_items)\n\ncolumns = [\"TId\", \"TDesc\", \"TDate\", \"ItemName\", \"Name\", \"City\", \"State\"]\n#Trans:        [\"TId\", \"StoreId\", \"ItemId\", \"TDesc\", \"TDate\"]\n#Trans+Items:  [\"TId\", \"ItemName\", \"TDesc\", \"TDate\"]\n#Store:        [\"Id\", \"Name\", \"City\", \"State\", \"Country\", \"Active\"]\ndf_trans_items_stores = df_trans_items.join(df_store, df_store.Id == df_trans_items.StoreId, \"inner\").select([col for col in columns])\ndf_trans_items_stores = df_trans_items_stores.withColumnRenamed(\"Name\", \"StoreName\")\ndf_trans_items_stores = df_trans_items_stores.withColumn(\"Location\", f.concat(df_trans_items_stores[\"City\"],df_trans_items_stores[\"State\"]))\nprint(\"df_trans_items_stores:\", df_trans_items_stores)\n\ndf_trans_items_stores.show()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f2168ea7-bd5e-477c-ba68-b29dd5c6073a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">================== DataFrame.JOIN ==================\ndf_trans_items: DataFrame[TId: bigint, TDesc: string, TDate: string, ItemName: string, StoreId: bigint]\ndf_trans_items_stores: DataFrame[TId: bigint, TDesc: string, TDate: string, ItemName: string, StoreName: string, City: string, State: string, Location: string]\n+---+--------------------+-------------------+--------+---------+----------+--------+-----------------+\n|TId|               TDesc|              TDate|ItemName|StoreName|      City|   State|         Location|\n+---+--------------------+-------------------+--------+---------+----------+--------+-----------------+\n|  1|Apples for StoreB...|2021-04-19T01:01:01|  Apples|  Store-A|  New York|New York| New YorkNew York|\n|  1|Bananas for Store...|2021-04-19T03:01:01| Bananas|  Store-B|Washington| Seattle|WashingtonSeattle|\n|  1|Oranges for Store...|2021-04-19T02:01:01| Organes|  Store-C|    Dallas|   Texas|      DallasTexas|\n+---+--------------------+-------------------+--------+---------+----------+--------+-----------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">================== DataFrame.JOIN ==================\ndf_trans_items: DataFrame[TId: bigint, TDesc: string, TDate: string, ItemName: string, StoreId: bigint]\ndf_trans_items_stores: DataFrame[TId: bigint, TDesc: string, TDate: string, ItemName: string, StoreName: string, City: string, State: string, Location: string]\n+---+--------------------+-------------------+--------+---------+----------+--------+-----------------+\nTId|               TDesc|              TDate|ItemName|StoreName|      City|   State|         Location|\n+---+--------------------+-------------------+--------+---------+----------+--------+-----------------+\n  1|Apples for StoreB...|2021-04-19T01:01:01|  Apples|  Store-A|  New York|New York| New YorkNew York|\n  1|Bananas for Store...|2021-04-19T03:01:01| Bananas|  Store-B|Washington| Seattle|WashingtonSeattle|\n  1|Oranges for Store...|2021-04-19T02:01:01| Organes|  Store-C|    Dallas|   Texas|      DallasTexas|\n+---+--------------------+-------------------+--------+---------+----------+--------+-----------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#SQL Style\ncolumns = [\"Id\", \"Name\", \"City\", \"State\", \"Country\", \"Active\"]\ndata = [\n  (1, \"Store-A\", \"New York\", \"New York\", \"USA\", 1),\n  (2, \"Store-B\", \"Washington\", \"Seattle\", \"USA\", 1),\n  (3, \"Store-C\", \"Dallas\", \"Texas\", \"USA\", 1)\n]\ndf_store = spark.createDataFrame(data= data, schema = columns )\nprint(\"df_store\")\ndf_store.show()\ndf_store.createOrReplaceTempView(\"stores\")\n\ncolumns = [\"Id\", \"Name\", \"Desc\", \"Active\"]\ndata = [(1, \"Bananas\", \"Bananas\", \"1\"),\n             (2,\"Apples\",\"Gala Apples\",\"1\"),\n             (3, \"Organes\", \"Naval Oranges\", \"1\")]\n\ndf_items = spark.createDataFrame(data= data, schema = columns )\nprint(\"df_items\")\ndf_items.show()\ndf_items.createOrReplaceTempView(\"items\")\n\n  \ncolumns = [\"TId\", \"StoreId\", \"ItemId\", \"TDesc\", \"TDate\"]\ndata = [(1, 1, 2, \"Apples for StoreB@WAS\", \"2021-04-19T01:01:01\"),\n             (1, 3, 3, \"Oranges for StoreC@DAL\", \"2021-04-19T02:01:01\"),\n             (1, 2, 1, \"Bananas for StoreA@NYC\", \"2021-04-19T03:01:01\")]\ndf_trans = spark.createDataFrame(data= data, schema = columns )\ndf_trans.createOrReplaceTempView(\"transactions\")\nprint(\"df_trans\")\ndf_trans.show()\n\nenrich_df = spark.sql(\"select t.TId, s.Name as Store_Name, i.Name as Item_name, t.TDesc, t.TDate, s.Active as Store_Active, i.Active as Item_Active, s.City from items i, stores s, transactions t where t.ItemId = i.Id and t.StoreId = s.Id\")\nprint(\"enrich_df::\", enrich_df)\nenrich_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc4e1844-3eea-47b4-9345-404232f3921c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark.sql.functions as f\nfrom pyspark.sql.types import StructType,StructField, StringType, LongType\n\n#Using WithColumn & UDF\nprint(\"================== DataFrame.WithColumn ==================\")\nprint(\"df_trans:\", df_trans)\n\ndef getItemValues(itemId):\n  print(\"getItemValues:\", type(itemId), itemId)\n  #getItemValues: <class 'pyspark.sql.column.Column'> Column<b'ItemId'>\n  #['{\"Id\":1,\"Name\":\"Bananas\",\"Desc\":\"Bananas\",\"Active\":\"1\"}']\n  \n  return itemId\n\nbc_items = spark.sparkContext.broadcast(df_items.toJSON().collect())        \ngetItemValues = udf(getItemValues, StringType())\ndf_new = df_trans.withColumn(\"ItemName\", getItemValues(df_trans.ItemId)) #f.lit(\"ITEM_NAME\")\ndf_new = df_new.drop(\"ItemId\")\n\ndef getStoreValues(storeId):\n  print(\"getStoreValues:\", type(storeId), storeId)\n  #getStoreValues: <class 'pyspark.sql.column.Column'> Column<b'StoreId'>\n  return storeId\n\ndf_store = spark.sparkContext.broadcast(df_store.toJSON().collect())\ngetStoreValues = udf(getStoreValues, StringType())\ndf_new = df_new.withColumn(\"StoreName\", f.lit(\"SITE_NAME\")) #getStoreValues(df_trans.StoreId, df_store)\ndf_new = df_new.drop(\"StoreId\")\n\nprint(\"df_enriched:2:\", df_new)\ndf_new.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5493756a-7771-4600-9253-9d9a8e8914e5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">================== DataFrame.WithColumn ==================\ndf_trans: DataFrame[TId: bigint, StoreId: bigint, ItemId: bigint, TDesc: string, TDate: string]\ndf_enriched:2: DataFrame[TId: bigint, TDesc: string, TDate: string, ItemName: string, StoreName: string]\n+---+--------------------+-------------------+--------+---------+\n|TId|               TDesc|              TDate|ItemName|StoreName|\n+---+--------------------+-------------------+--------+---------+\n|  1|Apples for StoreB...|2021-04-19T01:01:01|       2|SITE_NAME|\n|  1|Oranges for Store...|2021-04-19T02:01:01|       3|SITE_NAME|\n|  1|Bananas for Store...|2021-04-19T03:01:01|       1|SITE_NAME|\n+---+--------------------+-------------------+--------+---------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">================== DataFrame.WithColumn ==================\ndf_trans: DataFrame[TId: bigint, StoreId: bigint, ItemId: bigint, TDesc: string, TDate: string]\ndf_enriched:2: DataFrame[TId: bigint, TDesc: string, TDate: string, ItemName: string, StoreName: string]\n+---+--------------------+-------------------+--------+---------+\nTId|               TDesc|              TDate|ItemName|StoreName|\n+---+--------------------+-------------------+--------+---------+\n  1|Apples for StoreB...|2021-04-19T01:01:01|       2|SITE_NAME|\n  1|Oranges for Store...|2021-04-19T02:01:01|       3|SITE_NAME|\n  1|Bananas for Store...|2021-04-19T03:01:01|       1|SITE_NAME|\n+---+--------------------+-------------------+--------+---------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cab177c6-e491-4847-8ac8-8aed4f7ed475"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-functions-enrich","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3119029963728517}},"nbformat":4,"nbformat_minor":0}
