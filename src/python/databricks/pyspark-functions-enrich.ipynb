{"cells":[{"cell_type":"code","source":["pip install json5"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27ec5b96-7fcd-4be3-bc66-9bdf4d002925"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#Use case\n#Community Master Data: Users, Roles, UserRole\n#Transaction Data: Transactions (transId, userId, some details)\n#From userId, get the relevant username and perform anyother lookups\n\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col,sum,avg\nfrom pyspark.sql.types import Row\nimport json5 as json\n\ncolumns = [\"RId\",\"RName\", \"RActive\"]\ndata = [(1, \"Role1\", 1),\n    (2, \"Role2\", 1),\n    (3, \"Role3\", 1)]\ndf_role = spark.createDataFrame(data=data, schema=columns)\ndf_role.createOrReplaceTempView(\"roles\")\nbc_role = spark.sparkContext.broadcast(df_role.toJSON().collect())\n\ncolumns = [\"UId\",\"UName\", \"UActive\"]\ndata = [(1, \"User1\", 1),\n    (2, \"User2\", 1),\n    (3, \"User3\", 0),\n    (4, \"User4\", 1)]\ndf_user = spark.createDataFrame(data=data, schema=columns)\ndf_user.createOrReplaceTempView(\"users\")\nbc_user = spark.sparkContext.broadcast(df_user.toJSON().collect())\n\ncolumns = [\"UId\", \"RId\"]\ndata = [(1, 1), (1, 2), (2, 3), (3, 1), (3,2), (3,3), (4, 1), (4,3)]\ndf_user_role = spark.createDataFrame(data=data, schema=columns)\ndf_user_role.createOrReplaceTempView(\"user_role\")\nbc_user_role = spark.sparkContext.broadcast(df_user_role.toJSON().collect())\n\n\ndef myMapFunction(rec, bc_user, bc_role, bc_user_role):\n  print(\"myMapFunction::Type:%s, Data:%s\" % (type(rec), rec))\n  #myMapFunction::Type:<class 'pyspark.sql.types.Row'>, Data:Row(TId=4, UId=3, TDesc='TDesc4', TDate='2021-04-16T17:14:00')\n  row_dict = rec.asDict()\n  userId = row_dict.get(\"UId\")\n  print(\"myMapFunction::user:%s, bc_user:%s, bc_user.value:%s\" % (userId, type(bc_user), type(bc_user.value)))\n  enrich1_val = \"\"\n  for v in bc_user.value:\n    print(\"myMapFunction::type(v):%s, v:%s\" % (type(v), v))\n    #myMapFunction::type(v):<class 'str'>, v:{\"UId\":1,\"UName\":\"U1\",\"UActive\":1}\n    if type(v) == str:\n      v = json.loads(v)\n    if v.get(\"UId\") == userId:\n      enrich1_val = v.get(\"UName\")\n  #myMapFunction::user:3, bc_user:<class 'pyspark.broadcast.Broadcast'>, bc_user.value:<class 'list'>\n  row_dict[\"ENRICH_1\"] = enrich1_val #Adding a new column and setting some value.\n  print(\"myMapFunction::row_dict:%s\" % row_dict)\n  #myMapFunction::row_dict:{'TId': 4, 'UId': 3, 'TDesc': 'TDesc4', 'TDate': '2021-04-16T17:14:00', 'ENRICH_1': 'constant_value'}\n  return Row(**row_dict)\n\ncolumns = [\"TId\", \"UId\", \"TDesc\", \"TDate\"]\ndata = [\n  (1, 1, \"TDesc1\", \"2021-04-16T17:11:00\"), \n  (2, 1, \"TDesc2\", \"2021-04-16T17:12:00\"), \n  (3, 4, \"TDesc3\", \"2021-04-16T17:13:00\"), \n  (4, 3, \"TDesc4\", \"2021-04-16T17:14:00\"), \n  (5, 2, \"TDesc5\", \"2021-04-16T17:15:00\")]\ndf_trans = spark.createDataFrame(data=data, schema=columns)\n\ndf_new_rdd = df_trans.rdd.map(lambda r: myMapFunction(r, bc_user, bc_role, bc_user_role))\nprint(\"df_new_rdd:\", df_new_rdd)\ndf_new_rdd.toDF([\"TId\", \"UId\", \"TDesc\", \"TDate\", \"ENRICH_1\"]).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e9ba1dd9-41cc-41c1-bfba-e9f6a6c09423"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import StructType,StructField, StringType, LongType, IntegerType\n\ndef myFlatMapFunction(rec, bc_user, bc_role, bc_user_role):\n  print(\"myFlatMapFunction::Type:%s, Data:%s\" % (type(rec), rec))\n  #myFlatMapFunction::Type:<class 'pyspark.sql.types.Row'>, Data:Row(TId=4, UId=3, TDesc='TDesc4', TDate='2021-04-16T17:14:00')\n  return rec\n\ndf_new_rdd = df_trans.rdd.flatMap(lambda r: myFlatMapFunction(r, bc_user, bc_role, bc_user_role))\nprint(\"df_new_rdd:\", df_new_rdd)\n\ndef myForEach(rec):\n  print(\"myForEach::type:%s, rec:%s\" % (type(rec), rec))\n  #myForEach::type:<class 'int'>, rec:4\n  \n#Foreach\ndf_new_rdd.foreach(lambda r: myForEach(r))\n\nmyschema = StructType([       \n    StructField('TId', IntegerType(), True),\n    StructField('UId', IntegerType(), True),\n    StructField('TDesc', StringType(), True),\n    StructField('TDate', StringType(), True)\n])\ndf_new_rdd_df = spark.createDataFrame(df_new_rdd, schema = myschema)\ndf_new_rdd_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"836bad72-c31d-4a38-a55d-60de0880206a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["columns = [\"Id\", \"Name\", \"City\", \"State\", \"Country\", \"Active\"]\ndata = [\n  (1, \"Store-A\", \"New York\", \"New York\", \"USA\", 1),\n  (2, \"Store-B\", \"Washington\", \"Seattle\", \"USA\", 1),\n  (3, \"Store-C\", \"Dallas\", \"Texas\", \"USA\", 1)\n]\ndf_store = spark.createDataFrame(data= data, schema = columns )\nprint(\"df_store\")\ndf_store.show()\nbc_store = spark.sparkContext.broadcast(df_store.toJSON().collect())\n\ncolumns = [\"Id\", \"Name\", \"Desc\", \"Active\"]\ndata = [(1, \"Bananas\", \"Bananas\", \"1\"),\n             (2,\"Apples\",\"Gala Apples\",\"1\"),\n             (3, \"Organes\", \"Naval Oranges\", \"1\")]\n\ndf_items = spark.createDataFrame(data= data, schema = columns )\nprint(\"df_items\")\ndf_items.show()\nbc_items = spark.sparkContext.broadcast(df_items.toJSON().collect())\n\ndef myTransEnrich(row, bc_store, bc_items):\n  print(\"myTransEnrich::type:%s, data:%s\" % (type(row), row))\n  #myTransEnrich::type:<class 'pyspark.sql.types.Row'>, data:Row(TId=1, StoreId=1, ItemId=2, TDesc='Apples for StoreB@WAS', TDate='2021-04-19T01:01:01')\n  \n  return row\n  \ncolumns = [\"TId\", \"StoreId\", \"ItemId\", \"TDesc\", \"TDate\"]\ndata = [(1, 1, 2, \"Apples for StoreB@WAS\", \"2021-04-19T01:01:01\"),\n             (1, 3, 3, \"Oranges for StoreC@DAL\", \"2021-04-19T02:01:01\"),\n             (1, 2, 1, \"Bananas for StoreA@NYC\", \"2021-04-19T03:01:01\")]\ndf_trans = spark.createDataFrame(data= data, schema = columns )\nprint(\"df_trans\")\ndf_trans.show()\ndf_new_rdd = df_trans.rdd.map(lambda row: myTransEnrich(row, bc_store, bc_items))\nprint(\"df_new_rdd:\", df_new_rdd)\ndf_new_rdd.toDF([\"TId\", \"StoreId\", \"ItemId\", \"TDesc\", \"TDate\"]).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"540f2242-0d17-4afd-baff-b7b09c79d79e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">df_store\n+---+-------+----------+--------+-------+------+\n| Id|   Name|      City|   State|Country|Active|\n+---+-------+----------+--------+-------+------+\n|  1|Store-A|  New York|New York|    USA|     1|\n|  2|Store-B|Washington| Seattle|    USA|     1|\n|  3|Store-C|    Dallas|   Texas|    USA|     1|\n+---+-------+----------+--------+-------+------+\n\ndf_items\n+---+-------+-------------+------+\n| Id|   Name|         Desc|Active|\n+---+-------+-------------+------+\n|  1|Bananas|      Bananas|     1|\n|  2| Apples|  Gala Apples|     1|\n|  3|Organes|Naval Oranges|     1|\n+---+-------+-------------+------+\n\ndf_trans\n+---+-------+------+--------------------+-------------------+\n|TId|StoreId|ItemId|               TDesc|              TDate|\n+---+-------+------+--------------------+-------------------+\n|  1|      1|     2|Apples for StoreB...|2021-04-19T01:01:01|\n|  1|      3|     3|Oranges for Store...|2021-04-19T02:01:01|\n|  1|      2|     1|Bananas for Store...|2021-04-19T03:01:01|\n+---+-------+------+--------------------+-------------------+\n\ndf_new_rdd: PythonRDD[216] at RDD at PythonRDD.scala:58\n+---+-------+------+--------------------+-------------------+\n|TId|StoreId|ItemId|               TDesc|              TDate|\n+---+-------+------+--------------------+-------------------+\n|  1|      1|     2|Apples for StoreB...|2021-04-19T01:01:01|\n|  1|      3|     3|Oranges for Store...|2021-04-19T02:01:01|\n|  1|      2|     1|Bananas for Store...|2021-04-19T03:01:01|\n+---+-------+------+--------------------+-------------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">df_store\n+---+-------+----------+--------+-------+------+\n Id|   Name|      City|   State|Country|Active|\n+---+-------+----------+--------+-------+------+\n  1|Store-A|  New York|New York|    USA|     1|\n  2|Store-B|Washington| Seattle|    USA|     1|\n  3|Store-C|    Dallas|   Texas|    USA|     1|\n+---+-------+----------+--------+-------+------+\n\ndf_items\n+---+-------+-------------+------+\n Id|   Name|         Desc|Active|\n+---+-------+-------------+------+\n  1|Bananas|      Bananas|     1|\n  2| Apples|  Gala Apples|     1|\n  3|Organes|Naval Oranges|     1|\n+---+-------+-------------+------+\n\ndf_trans\n+---+-------+------+--------------------+-------------------+\nTId|StoreId|ItemId|               TDesc|              TDate|\n+---+-------+------+--------------------+-------------------+\n  1|      1|     2|Apples for StoreB...|2021-04-19T01:01:01|\n  1|      3|     3|Oranges for Store...|2021-04-19T02:01:01|\n  1|      2|     1|Bananas for Store...|2021-04-19T03:01:01|\n+---+-------+------+--------------------+-------------------+\n\ndf_new_rdd: PythonRDD[216] at RDD at PythonRDD.scala:58\n+---+-------+------+--------------------+-------------------+\nTId|StoreId|ItemId|               TDesc|              TDate|\n+---+-------+------+--------------------+-------------------+\n  1|      1|     2|Apples for StoreB...|2021-04-19T01:01:01|\n  1|      3|     3|Oranges for Store...|2021-04-19T02:01:01|\n  1|      2|     1|Bananas for Store...|2021-04-19T03:01:01|\n+---+-------+------+--------------------+-------------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc4e1844-3eea-47b4-9345-404232f3921c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-functions-enrich","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3119029963728517}},"nbformat":4,"nbformat_minor":0}
