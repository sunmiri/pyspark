{"cells":[{"cell_type":"code","source":["pip install json5"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27ec5b96-7fcd-4be3-bc66-9bdf4d002925"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#Use case\n#Community Master Data: Users, Roles, UserRole\n#Transaction Data: Transactions (transId, userId, some details)\n#From userId, get the relevant username and perform anyother lookups\n\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col,sum,avg\nfrom pyspark.sql.types import Row\nimport json5 as json\n\ncolumns = [\"RId\",\"RName\", \"RActive\"]\ndata = [(1, \"Role1\", 1),\n    (2, \"Role2\", 1),\n    (3, \"Role3\", 1)]\ndf_role = spark.createDataFrame(data=data, schema=columns)\ndf_role.createOrReplaceTempView(\"roles\")\nbc_role = spark.sparkContext.broadcast(df_role.toJSON().collect())\n\ncolumns = [\"UId\",\"UName\", \"UActive\"]\ndata = [(1, \"User1\", 1),\n    (2, \"User2\", 1),\n    (3, \"User3\", 0),\n    (4, \"User4\", 1)]\ndf_user = spark.createDataFrame(data=data, schema=columns)\ndf_user.createOrReplaceTempView(\"users\")\nbc_user = spark.sparkContext.broadcast(df_user.toJSON().collect())\n\ncolumns = [\"UId\", \"RId\"]\ndata = [(1, 1), (1, 2), (2, 3), (3, 1), (3,2), (3,3), (4, 1), (4,3)]\ndf_user_role = spark.createDataFrame(data=data, schema=columns)\ndf_user_role.createOrReplaceTempView(\"user_role\")\nbc_user_role = spark.sparkContext.broadcast(df_user_role.toJSON().collect())\n\n\ndef myMapFunction(rec, bc_user, bc_role, bc_user_role):\n  print(\"myMapFunction::Type:%s, Data:%s\" % (type(rec), rec))\n  #myMapFunction::Type:<class 'pyspark.sql.types.Row'>, Data:Row(TId=4, UId=3, TDesc='TDesc4', TDate='2021-04-16T17:14:00')\n  row_dict = rec.asDict()\n  userId = row_dict.get(\"UId\")\n  print(\"myMapFunction::user:%s, bc_user:%s, bc_user.value:%s\" % (userId, type(bc_user), type(bc_user.value)))\n  enrich1_val = \"\"\n  for v in bc_user.value:\n    print(\"myMapFunction::type(v):%s, v:%s\" % (type(v), v))\n    #myMapFunction::type(v):<class 'str'>, v:{\"UId\":1,\"UName\":\"U1\",\"UActive\":1}\n    if type(v) == str:\n      v = json.loads(v)\n    if v.get(\"UId\") == userId:\n      enrich1_val = v.get(\"UName\")\n  #myMapFunction::user:3, bc_user:<class 'pyspark.broadcast.Broadcast'>, bc_user.value:<class 'list'>\n  row_dict[\"ENRICH_1\"] = enrich1_val #Adding a new column and setting some value.\n  print(\"myMapFunction::row_dict:%s\" % row_dict)\n  #myMapFunction::row_dict:{'TId': 4, 'UId': 3, 'TDesc': 'TDesc4', 'TDate': '2021-04-16T17:14:00', 'ENRICH_1': 'constant_value'}\n  return Row(**row_dict)\n\ncolumns = [\"TId\", \"UId\", \"TDesc\", \"TDate\"]\ndata = [\n  (1, 1, \"TDesc1\", \"2021-04-16T17:11:00\"), \n  (2, 1, \"TDesc2\", \"2021-04-16T17:12:00\"), \n  (3, 4, \"TDesc3\", \"2021-04-16T17:13:00\"), \n  (4, 3, \"TDesc4\", \"2021-04-16T17:14:00\"), \n  (5, 2, \"TDesc5\", \"2021-04-16T17:15:00\")]\ndf_trans = spark.createDataFrame(data=data, schema=columns)\n\ndf_new_rdd = df_trans.rdd.map(lambda r: myMapFunction(r, bc_user, bc_role, bc_user_role))\nprint(\"df_new_rdd:\", df_new_rdd)\ndf_new_rdd.toDF([\"TId\", \"UId\", \"TDesc\", \"TDate\", \"ENRICH_1\"]).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e9ba1dd9-41cc-41c1-bfba-e9f6a6c09423"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import StructType,StructField, StringType, LongType\n\ndef myFlatMapFunction(rec):\n  print(\"myFlatMapFunction::Type:%s, Data:%s\" % (type(rec), rec))\n  #myFlatMapFunction::Type:<class 'pyspark.sql.types.Row'>, Data:Row(TId=4, UId=3, TDesc='TDesc4', TDate='2021-04-16T17:14:00')\n  return rec\n\nprint(\"Source Dataframe::df_trans:\", df_trans)\ndf_trans.printSchema()\ndf_trans.show()\ndf_new_rdd = df_trans.rdd.flatMap(lambda r: myFlatMapFunction(r))\nprint(\"df_new_rdd:\", df_new_rdd)\n#df_new_rdd.toDF() #Can not infer schema for type: <class 'int'>\n\ndef myForEach(rec):\n  print(\"myForEach::type:%s, rec:%s\" % (type(rec), rec))\n  #myForEach::type:<class 'int'>, rec:4\n\n#Foreach\ndf_new_rdd.foreach(lambda r: myForEach(r))\n\nmyschema = StructType([       \n    StructField('TId', LongType(), True),\n    StructField('UId', LongType(), True),\n    StructField('TDesc', StringType(), True),\n    StructField('TDate', StringType(), True)\n])\ndf_new_rdd_df = spark.createDataFrame(df_new_rdd, schema = myschema)\nprint(\"RDD to DF::df_new_rdd_df:\", df_new_rdd_df)\ndf_new_rdd_df.printSchema()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"836bad72-c31d-4a38-a55d-60de0880206a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark.sql.functions as f\n\ncolumns = [\"Id\", \"Name\", \"City\", \"State\", \"Country\", \"Active\"]\ndata = [\n  (1, \"Store-A\", \"New York\", \"New York\", \"USA\", 1),\n  (2, \"Store-B\", \"Washington\", \"Seattle\", \"USA\", 1),\n  (3, \"Store-C\", \"Dallas\", \"Texas\", \"USA\", 1)\n]\ndf_store = spark.createDataFrame(data= data, schema = columns )\nprint(\"df_store\")\ndf_store.show()\nbc_store = spark.sparkContext.broadcast(df_store.toJSON().collect())\n\ncolumns = [\"Id\", \"Name\", \"Desc\", \"Active\"]\ndata = [(1, \"Bananas\", \"Bananas\", \"1\"),\n             (2,\"Apples\",\"Gala Apples\",\"1\"),\n             (3, \"Organes\", \"Naval Oranges\", \"1\")]\n\ndf_items = spark.createDataFrame(data= data, schema = columns )\nprint(\"df_items\")\ndf_items.show()\nbc_items = spark.sparkContext.broadcast(df_items.toJSON().collect())\n\ndef myTransEnrich(row, bc_store, bc_items):\n  print(\"myTransEnrich::type:%s, data:%s\" % (type(row), row))\n  #myTransEnrich::type:<class 'pyspark.sql.types.Row'>, data:Row(TId=1, StoreId=1, ItemId=2, TDesc='Apples for StoreB@WAS', TDate='2021-04-19T01:01:01')\n  \n  return row\n  \ncolumns = [\"TId\", \"StoreId\", \"ItemId\", \"TDesc\", \"TDate\"]\ndata = [(1, 1, 2, \"Apples for StoreB@WAS\", \"2021-04-19T01:01:01\"),\n             (1, 3, 3, \"Oranges for StoreC@DAL\", \"2021-04-19T02:01:01\"),\n             (1, 2, 1, \"Bananas for StoreA@NYC\", \"2021-04-19T03:01:01\")]\ndf_trans = spark.createDataFrame(data= data, schema = columns )\nprint(\"df_trans\")\ndf_trans.show()\n\n#Using Map\ndf_enriched = df_trans.rdd.map(lambda row: myTransEnrich(row, bc_store, bc_items))\nprint(\"df_enriched:0:\", df_enriched)\ndf_enriched.toDF([\"TId\", \"StoreId\", \"ItemId\", \"TDesc\", \"TDate\"]).show()\n\n#using dataframe joins\ncolumns = [\"TId\", \"TDesc\", \"TDate\", \"Name\"]\ndf_trans_items = df_trans.join(df_items, df_items.Id == df_trans.ItemId, \"inner\").select([col for col in columns])\ndf_trans_items.withColumnRenamed(\"Name\", \"ItemName\")\nprint(\"df_trans_items:\", df_trans_items)\n\ncolumns = [\"TId\", \"TDesc\", \"TDate\", \"Name\"]\ndf_trans_stores = df_trans.join(df_store, df_store.Id == df_trans.StoreId, \"inner\").select([col for col in columns])\ndf_trans_stores.withColumnRenamed(\"Name\", \"StoreName\")\nprint(\"df_trans_stores:\", df_trans_stores)\n\ndf_enriched = df_trans_items.join(df_trans_stores, df_trans_items.TId == df_trans_stores.TId, \"inner\")\nprint(\"df_enriched:1:\", df_enriched)\ndf_enriched.show()\n\n#Using WithColumn & UDF\nprint(\"df_trans:\", df_trans)\n\ndef getItemValues(itemId):\n  print(\"getItemValues:\", type(itemId), itemId)\n  #getItemValues: <class 'pyspark.sql.column.Column'> Column<b'ItemId'>\n  return itemId\n  \ndf_new = df_trans.withColumn(\"ItemName\", getItemValues(df_trans.ItemId))\ndf_new = df_new.drop(\"ItemId\")\ndef getStoreValues(storeId):\n  print(\"getStoreValues:\", type(storeId), storeId)\n  #getStoreValues: <class 'pyspark.sql.column.Column'> Column<b'StoreId'>\n  return storeId\ndf_new = df_new.withColumn(\"StoreName\", getStoreValues(df_trans.StoreId))\ndf_new = df_new.drop(\"StoreId\")\n\nprint(\"df_enriched:2:\", df_new)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"540f2242-0d17-4afd-baff-b7b09c79d79e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#SQL Style\ncolumns = [\"Id\", \"Name\", \"City\", \"State\", \"Country\", \"Active\"]\ndata = [\n  (1, \"Store-A\", \"New York\", \"New York\", \"USA\", 1),\n  (2, \"Store-B\", \"Washington\", \"Seattle\", \"USA\", 1),\n  (3, \"Store-C\", \"Dallas\", \"Texas\", \"USA\", 1)\n]\ndf_store = spark.createDataFrame(data= data, schema = columns )\nprint(\"df_store\")\ndf_store.show()\ndf_store.createOrReplaceTempView(\"stores\")\n\ncolumns = [\"Id\", \"Name\", \"Desc\", \"Active\"]\ndata = [(1, \"Bananas\", \"Bananas\", \"1\"),\n             (2,\"Apples\",\"Gala Apples\",\"1\"),\n             (3, \"Organes\", \"Naval Oranges\", \"1\")]\n\ndf_items = spark.createDataFrame(data= data, schema = columns )\nprint(\"df_items\")\ndf_items.show()\ndf_items.createOrReplaceTempView(\"items\")\n\n  \ncolumns = [\"TId\", \"StoreId\", \"ItemId\", \"TDesc\", \"TDate\"]\ndata = [(1, 1, 2, \"Apples for StoreB@WAS\", \"2021-04-19T01:01:01\"),\n             (1, 3, 3, \"Oranges for StoreC@DAL\", \"2021-04-19T02:01:01\"),\n             (1, 2, 1, \"Bananas for StoreA@NYC\", \"2021-04-19T03:01:01\")]\ndf_trans = spark.createDataFrame(data= data, schema = columns )\ndf_trans.createOrReplaceTempView(\"transactions\")\nprint(\"df_trans\")\ndf_trans.show()\n\nenrich_df = spark.sql(\"select t.TId, s.Name as Store_Name, i.Name as Item_name, t.TDesc, t.TDate, s.Active as Store_Active, i.Active as Item_Active, s.City from items i, stores s, transactions t where t.ItemId = i.Id and t.StoreId = s.Id\")\nprint(\"enrich_df::\", enrich_df)\nenrich_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc4e1844-3eea-47b4-9345-404232f3921c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5493756a-7771-4600-9253-9d9a8e8914e5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-functions-enrich","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3119029963728517}},"nbformat":4,"nbformat_minor":0}
