{"cells":[{"cell_type":"code","source":["pip install json5"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27ec5b96-7fcd-4be3-bc66-9bdf4d002925"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting json5\n  Downloading json5-0.9.5-py2.py3-none-any.whl (17 kB)\nInstalling collected packages: json5\nSuccessfully installed json5-0.9.5\nPython interpreter will be restarted.\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting json5\n  Downloading json5-0.9.5-py2.py3-none-any.whl (17 kB)\nInstalling collected packages: json5\nSuccessfully installed json5-0.9.5\nPython interpreter will be restarted.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#Use case\n#Community Master Data: Users, Roles, UserRole\n#Transaction Data: Transactions (transId, userId, some details)\n#From userId, get the relevant username and perform anyother lookups\n\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col,sum,avg\nfrom pyspark.sql.types import Row\nimport json5 as json\n\ncolumns = [\"RId\",\"RName\", \"RActive\"]\ndata = [(1, \"Role1\", 1),\n    (2, \"Role2\", 1),\n    (3, \"Role3\", 1)]\ndf_role = spark.createDataFrame(data=data, schema=columns)\ndf_role.createOrReplaceTempView(\"roles\")\nbc_role = spark.sparkContext.broadcast(df_role.toJSON().collect())\n\ncolumns = [\"UId\",\"UName\", \"UActive\"]\ndata = [(1, \"User1\", 1),\n    (2, \"User2\", 1),\n    (3, \"User3\", 0),\n    (4, \"User4\", 1)]\ndf_user = spark.createDataFrame(data=data, schema=columns)\ndf_user.createOrReplaceTempView(\"users\")\nbc_user = spark.sparkContext.broadcast(df_user.toJSON().collect())\n\ncolumns = [\"UId\", \"RId\"]\ndata = [(1, 1), (1, 2), (2, 3), (3, 1), (3,2), (3,3), (4, 1), (4,3)]\ndf_user_role = spark.createDataFrame(data=data, schema=columns)\ndf_user_role.createOrReplaceTempView(\"user_role\")\nbc_user_role = spark.sparkContext.broadcast(df_user_role.toJSON().collect())\n\n\ndef myMapFunction(rec, bc_user, bc_role, bc_user_role):\n  print(\"myMapFunction::Type:%s, Data:%s\" % (type(rec), rec))\n  #myMapFunction::Type:<class 'pyspark.sql.types.Row'>, Data:Row(TId=4, UId=3, TDesc='TDesc4', TDate='2021-04-16T17:14:00')\n  row_dict = rec.asDict()\n  userId = row_dict.get(\"UId\")\n  print(\"myMapFunction::user:%s, bc_user:%s, bc_user.value:%s\" % (userId, type(bc_user), type(bc_user.value)))\n  enrich1_val = \"\"\n  for v in bc_user.value:\n    print(\"myMapFunction::type(v):%s, v:%s\" % (type(v), v))\n    #myMapFunction::type(v):<class 'str'>, v:{\"UId\":1,\"UName\":\"U1\",\"UActive\":1}\n    if type(v) == str:\n      v = json.loads(v)\n    if v.get(\"UId\") == userId:\n      enrich1_val = v.get(\"UName\")\n  #myMapFunction::user:3, bc_user:<class 'pyspark.broadcast.Broadcast'>, bc_user.value:<class 'list'>\n  row_dict[\"ENRICH_1\"] = enrich1_val #Adding a new column and setting some value.\n  print(\"myMapFunction::row_dict:%s\" % row_dict)\n  #myMapFunction::row_dict:{'TId': 4, 'UId': 3, 'TDesc': 'TDesc4', 'TDate': '2021-04-16T17:14:00', 'ENRICH_1': 'constant_value'}\n  return Row(**row_dict)\n\ncolumns = [\"TId\", \"UId\", \"TDesc\", \"TDate\"]\ndata = [\n  (1, 1, \"TDesc1\", \"2021-04-16T17:11:00\"), \n  (2, 1, \"TDesc2\", \"2021-04-16T17:12:00\"), \n  (3, 4, \"TDesc3\", \"2021-04-16T17:13:00\"), \n  (4, 3, \"TDesc4\", \"2021-04-16T17:14:00\"), \n  (5, 2, \"TDesc5\", \"2021-04-16T17:15:00\")]\ndf_trans = spark.createDataFrame(data=data, schema=columns)\n\ndf_new_rdd = df_trans.rdd.map(lambda r: myMapFunction(r, bc_user, bc_role, bc_user_role))\nprint(\"df_new_rdd:\", df_new_rdd)\ndf_new_rdd.toDF([\"TId\", \"UId\", \"TDesc\", \"TDate\", \"ENRICH_1\"]).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e9ba1dd9-41cc-41c1-bfba-e9f6a6c09423"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import StructType,StructField, StringType, LongType\n\ndef myFlatMapFunction(rec):\n  print(\"myFlatMapFunction::Type:%s, Data:%s\" % (type(rec), rec))\n  #myFlatMapFunction::Type:<class 'pyspark.sql.types.Row'>, Data:Row(TId=4, UId=3, TDesc='TDesc4', TDate='2021-04-16T17:14:00')\n  return rec\n\nprint(\"Source Dataframe::df_trans:\", df_trans)\ndf_trans.printSchema()\ndf_trans.show()\ndf_new_rdd = df_trans.rdd.flatMap(lambda r: myFlatMapFunction(r))\nprint(\"df_new_rdd:\", df_new_rdd)\n#df_new_rdd.toDF() #Can not infer schema for type: <class 'int'>\n\ndef myForEach(rec):\n  print(\"myForEach::type:%s, rec:%s\" % (type(rec), rec))\n  #myForEach::type:<class 'int'>, rec:4\n\n#Foreach\ndf_new_rdd.foreach(lambda r: myForEach(r))\n\nmyschema = StructType([       \n    StructField('TId', LongType(), True),\n    StructField('UId', LongType(), True),\n    StructField('TDesc', StringType(), True),\n    StructField('TDate', StringType(), True)\n])\ndf_new_rdd_df = spark.createDataFrame(df_new_rdd, schema = myschema)\nprint(\"RDD to DF::df_new_rdd_df:\", df_new_rdd_df)\ndf_new_rdd_df.printSchema()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"836bad72-c31d-4a38-a55d-60de0880206a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark.sql.functions as f\n\ncolumns = [\"Id\", \"Name\", \"City\", \"State\", \"Country\", \"Active\"]\ndata = [\n  (1, \"Store-A\", \"New York\", \"New York\", \"USA\", 1),\n  (2, \"Store-B\", \"Washington\", \"Seattle\", \"USA\", 1),\n  (3, \"Store-C\", \"Dallas\", \"Texas\", \"USA\", 1)\n]\ndf_store = spark.createDataFrame(data= data, schema = columns )\nprint(\"df_store\")\ndf_store.show()\nbc_store = spark.sparkContext.broadcast(df_store.toJSON().collect())\n\ncolumns = [\"Id\", \"Name\", \"Desc\", \"Active\"]\ndata = [(1, \"Bananas\", \"Bananas\", \"1\"),\n             (2,\"Apples\",\"Gala Apples\",\"1\"),\n             (3, \"Organes\", \"Naval Oranges\", \"1\")]\n\ndf_items = spark.createDataFrame(data= data, schema = columns )\nprint(\"df_items\")\ndf_items.show()\nbc_items = spark.sparkContext.broadcast(df_items.toJSON().collect())\nprint(\"df_items.toJSON().collect():\", df_items.toJSON().collect())\n#df_items.toJSON().collect(): ['{\"Id\":1,\"Name\":\"Bananas\",\"Desc\":\"Bananas\",\"Active\":\"1\"}']\n\ndef myTransEnrich(row, bc_store, bc_items):\n  print(\"myTransEnrich::type:%s, data:%s\" % (type(row), row))\n  #myTransEnrich::type:<class 'pyspark.sql.types.Row'>, data:Row(TId=1, StoreId=1, ItemId=2, TDesc='Apples for StoreB@WAS', TDate='2021-04-19T01:01:01')\n  print(\"myTransEnrich::bc_user.value::\", type(bc_store.value), bc_store.value)\n  if type(bc_store.value) == list and len(bc_store.value) > 0:\n    json_stores = bc_store.value[0]\n    if type(json_stores) == str:\n      json_stores = json.loads(json_stores)\n  #myTransEnrich::bc_user.value:: <class 'list'> ['{\"Id\":1,\"Name\":\"Store-A\",\"City\":\"New York\",\"State\":\"New York\",\"Country\":\"USA\",\"Active\":1}', '{\"Id\":2,\"Name\":\"Store-B\",\"City\":\"Washington\",\"State\":\"Seattle\",\"Country\":\"USA\",\"Active\":1}', '{\"Id\":3,\"Name\":\"Store-C\",\"City\":\"Dallas\",\"State\":\"Texas\",\"Country\":\"USA\",\"Active\":1}']\n\n  print(\"myTransEnrich::bc_items.value::\", type(bc_items.value), bc_items.value)\n  if type(bc_items.value) == list and len(bc_items.value) > 0:\n    json_items = bc_items.value[0]\n    if type(json_items) == str:\n      json_items = json.loads(json_items)\n  #myTransEnrich::bc_items.value:: <class 'list'> ['{\"Id\":1,\"Name\":\"Bananas\",\"Desc\":\"Bananas\",\"Active\":\"1\"}', '{\"Id\":2,\"Name\":\"Apples\",\"Desc\":\"Gala Apples\",\"Active\":\"1\"}', '{\"Id\":3,\"Name\":\"Organes\",\"Desc\":\"Naval Oranges\",\"Active\":\"1\"}']\n        \n  #Store for-loop\n  #Items for-loop\n  return row\n  \ncolumns = [\"TId\", \"StoreId\", \"ItemId\", \"TDesc\", \"TDate\"]\ndata = [(1, 1, 2, \"Apples for StoreB@WAS\", \"2021-04-19T01:01:01\"),\n             (1, 3, 3, \"Oranges for StoreC@DAL\", \"2021-04-19T02:01:01\"),\n             (1, 2, 1, \"Bananas for StoreA@NYC\", \"2021-04-19T03:01:01\")]\ndf_trans = spark.createDataFrame(data= data, schema = columns )\nprint(\"df_trans\")\ndf_trans.show()\n\n#Using Map\ndf_enriched = df_trans.rdd.map(lambda row: myTransEnrich(row, bc_store, bc_items))\nprint(\"df_enriched:0:\", df_enriched)\ndf_enriched.toDF([\"TId\", \"StoreId\", \"ItemId\", \"TDesc\", \"TDate\"]).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"540f2242-0d17-4afd-baff-b7b09c79d79e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">df_store\n+---+-------+----------+--------+-------+------+\n| Id|   Name|      City|   State|Country|Active|\n+---+-------+----------+--------+-------+------+\n|  1|Store-A|  New York|New York|    USA|     1|\n|  2|Store-B|Washington| Seattle|    USA|     1|\n|  3|Store-C|    Dallas|   Texas|    USA|     1|\n+---+-------+----------+--------+-------+------+\n\ndf_items\n+---+-------+-------------+------+\n| Id|   Name|         Desc|Active|\n+---+-------+-------------+------+\n|  1|Bananas|      Bananas|     1|\n|  2| Apples|  Gala Apples|     1|\n|  3|Organes|Naval Oranges|     1|\n+---+-------+-------------+------+\n\ndf_items.toJSON().collect(): [&#39;{&#34;Id&#34;:1,&#34;Name&#34;:&#34;Bananas&#34;,&#34;Desc&#34;:&#34;Bananas&#34;,&#34;Active&#34;:&#34;1&#34;}&#39;, &#39;{&#34;Id&#34;:2,&#34;Name&#34;:&#34;Apples&#34;,&#34;Desc&#34;:&#34;Gala Apples&#34;,&#34;Active&#34;:&#34;1&#34;}&#39;, &#39;{&#34;Id&#34;:3,&#34;Name&#34;:&#34;Organes&#34;,&#34;Desc&#34;:&#34;Naval Oranges&#34;,&#34;Active&#34;:&#34;1&#34;}&#39;]\ndf_trans\n+---+-------+------+--------------------+-------------------+\n|TId|StoreId|ItemId|               TDesc|              TDate|\n+---+-------+------+--------------------+-------------------+\n|  1|      1|     2|Apples for StoreB...|2021-04-19T01:01:01|\n|  1|      3|     3|Oranges for Store...|2021-04-19T02:01:01|\n|  1|      2|     1|Bananas for Store...|2021-04-19T03:01:01|\n+---+-------+------+--------------------+-------------------+\n\ndf_enriched:0: PythonRDD[547] at RDD at PythonRDD.scala:58\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">df_store\n+---+-------+----------+--------+-------+------+\n Id|   Name|      City|   State|Country|Active|\n+---+-------+----------+--------+-------+------+\n  1|Store-A|  New York|New York|    USA|     1|\n  2|Store-B|Washington| Seattle|    USA|     1|\n  3|Store-C|    Dallas|   Texas|    USA|     1|\n+---+-------+----------+--------+-------+------+\n\ndf_items\n+---+-------+-------------+------+\n Id|   Name|         Desc|Active|\n+---+-------+-------------+------+\n  1|Bananas|      Bananas|     1|\n  2| Apples|  Gala Apples|     1|\n  3|Organes|Naval Oranges|     1|\n+---+-------+-------------+------+\n\ndf_items.toJSON().collect(): [&#39;{&#34;Id&#34;:1,&#34;Name&#34;:&#34;Bananas&#34;,&#34;Desc&#34;:&#34;Bananas&#34;,&#34;Active&#34;:&#34;1&#34;}&#39;, &#39;{&#34;Id&#34;:2,&#34;Name&#34;:&#34;Apples&#34;,&#34;Desc&#34;:&#34;Gala Apples&#34;,&#34;Active&#34;:&#34;1&#34;}&#39;, &#39;{&#34;Id&#34;:3,&#34;Name&#34;:&#34;Organes&#34;,&#34;Desc&#34;:&#34;Naval Oranges&#34;,&#34;Active&#34;:&#34;1&#34;}&#39;]\ndf_trans\n+---+-------+------+--------------------+-------------------+\nTId|StoreId|ItemId|               TDesc|              TDate|\n+---+-------+------+--------------------+-------------------+\n  1|      1|     2|Apples for StoreB...|2021-04-19T01:01:01|\n  1|      3|     3|Oranges for Store...|2021-04-19T02:01:01|\n  1|      2|     1|Bananas for Store...|2021-04-19T03:01:01|\n+---+-------+------+--------------------+-------------------+\n\ndf_enriched:0: PythonRDD[547] at RDD at PythonRDD.scala:58\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-567969701946348&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     56</span> df_enriched <span class=\"ansi-blue-fg\">=</span> df_trans<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> row<span class=\"ansi-blue-fg\">:</span> myTransEnrich<span class=\"ansi-blue-fg\">(</span>row<span class=\"ansi-blue-fg\">,</span> bc_store<span class=\"ansi-blue-fg\">,</span> bc_items<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     57</span> print<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;df_enriched:0:&#34;</span><span class=\"ansi-blue-fg\">,</span> df_enriched<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">---&gt; 58</span><span class=\"ansi-red-fg\"> </span>df_enriched<span class=\"ansi-blue-fg\">.</span>toDF<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;TId&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;StoreId&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;ItemId&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;TDesc&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;TDate&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">toDF</span><span class=\"ansi-blue-fg\">(self, schema, sampleRatio)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-blue-fg\">[</span>Row<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;Alice&#39;</span><span class=\"ansi-blue-fg\">,</span> age<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     63</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">---&gt; 64</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> sparkSession<span class=\"ansi-blue-fg\">.</span>createDataFrame<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> sampleRatio<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     65</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     66</span>     RDD<span class=\"ansi-blue-fg\">.</span>toDF <span class=\"ansi-blue-fg\">=</span> toDF\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/databricks/utils/instrumentation.py</span> in <span class=\"ansi-cyan-fg\">wrapper</span><span class=\"ansi-blue-fg\">(self, *args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     40</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     41</span>             start_time <span class=\"ansi-blue-fg\">=</span> time<span class=\"ansi-blue-fg\">.</span>time<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">---&gt; 42</span><span class=\"ansi-red-fg\">             </span>return_val <span class=\"ansi-blue-fg\">=</span> func<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kwargs<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     43</span>         <span class=\"ansi-green-fg\">except</span> Exception<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     44</span>             duration <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">(</span>time<span class=\"ansi-blue-fg\">.</span>time<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">-</span> start_time<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">*</span> <span class=\"ansi-cyan-fg\">1000</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">createDataFrame</span><span class=\"ansi-blue-fg\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    656</span>             return super(SparkSession, self).createDataFrame(\n<span class=\"ansi-green-intense-fg ansi-bold\">    657</span>                 data, schema, samplingRatio, verifySchema)\n<span class=\"ansi-green-fg\">--&gt; 658</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_create_dataframe<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> verifySchema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    659</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    660</span>     <span class=\"ansi-green-fg\">def</span> _create_dataframe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> verifySchema<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_create_dataframe</span><span class=\"ansi-blue-fg\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    686</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    687</span>             <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> RDD<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 688</span><span class=\"ansi-red-fg\">                 </span>rdd<span class=\"ansi-blue-fg\">,</span> schema <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_createFromRDD<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span>prepare<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    689</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    690</span>                 rdd<span class=\"ansi-blue-fg\">,</span> schema <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_createFromLocal<span class=\"ansi-blue-fg\">(</span>map<span class=\"ansi-blue-fg\">(</span>prepare<span class=\"ansi-blue-fg\">,</span> data<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_createFromRDD</span><span class=\"ansi-blue-fg\">(self, rdd, schema, samplingRatio)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    429</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    430</span>         <span class=\"ansi-green-fg\">if</span> schema <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">None</span> <span class=\"ansi-green-fg\">or</span> isinstance<span class=\"ansi-blue-fg\">(</span>schema<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">(</span>list<span class=\"ansi-blue-fg\">,</span> tuple<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 431</span><span class=\"ansi-red-fg\">             </span>struct <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_inferSchema<span class=\"ansi-blue-fg\">(</span>rdd<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> names<span class=\"ansi-blue-fg\">=</span>schema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    432</span>             converter <span class=\"ansi-blue-fg\">=</span> _create_converter<span class=\"ansi-blue-fg\">(</span>struct<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    433</span>             rdd <span class=\"ansi-blue-fg\">=</span> rdd<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span>converter<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_inferSchema</span><span class=\"ansi-blue-fg\">(self, rdd, samplingRatio, names)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    400</span>         <span class=\"ansi-blue-fg\">:</span><span class=\"ansi-green-fg\">return</span><span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">:</span><span class=\"ansi-green-fg\">class</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-red-fg\">`</span>pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>types<span class=\"ansi-blue-fg\">.</span>StructType<span class=\"ansi-red-fg\">`</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    401</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 402</span><span class=\"ansi-red-fg\">         </span>first <span class=\"ansi-blue-fg\">=</span> rdd<span class=\"ansi-blue-fg\">.</span>first<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    403</span>         <span class=\"ansi-green-fg\">if</span> <span class=\"ansi-green-fg\">not</span> first<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    404</span>             raise ValueError(&#34;The first row in RDD is empty, &#34;\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">first</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1491</span>         ValueError<span class=\"ansi-blue-fg\">:</span> RDD <span class=\"ansi-green-fg\">is</span> empty\n<span class=\"ansi-green-intense-fg ansi-bold\">   1492</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">-&gt; 1493</span><span class=\"ansi-red-fg\">         </span>rs <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>take<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1494</span>         <span class=\"ansi-green-fg\">if</span> rs<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1495</span>             <span class=\"ansi-green-fg\">return</span> rs<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">take</span><span class=\"ansi-blue-fg\">(self, num)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1473</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1474</span>             p <span class=\"ansi-blue-fg\">=</span> range<span class=\"ansi-blue-fg\">(</span>partsScanned<span class=\"ansi-blue-fg\">,</span> min<span class=\"ansi-blue-fg\">(</span>partsScanned <span class=\"ansi-blue-fg\">+</span> numPartsToTry<span class=\"ansi-blue-fg\">,</span> totalParts<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1475</span><span class=\"ansi-red-fg\">             </span>res <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">.</span>runJob<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> takeUpToNumLeft<span class=\"ansi-blue-fg\">,</span> p<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1476</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1477</span>             items <span class=\"ansi-blue-fg\">+=</span> res\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/context.py</span> in <span class=\"ansi-cyan-fg\">runJob</span><span class=\"ansi-blue-fg\">(self, rdd, partitionFunc, partitions, allowLocal)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1227</span>             <span class=\"ansi-green-fg\">finally</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1228</span>                 os<span class=\"ansi-blue-fg\">.</span>remove<span class=\"ansi-blue-fg\">(</span>filename<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1229</span><span class=\"ansi-red-fg\">         </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>runJob<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jsc<span class=\"ansi-blue-fg\">.</span>sc<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> mappedRDD<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">,</span> partitions<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1230</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> mappedRDD<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1231</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1304</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1305</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1307</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    126</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 127</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    128</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    129</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 235.0 failed 1 times, most recent failure: Lost task 1.0 in stage 235.0 (TID 760, ip-10-172-247-115.us-west-2.compute.internal, executor driver): org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;json&#39; is not defined&#39;, from &lt;command-567969701946348&gt;, line 32. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 676, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 668, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 279, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 1469, in takeUpToNumLeft\n    yield next(iterator)\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 110, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-567969701946348&gt;&#34;, line 56, in &lt;lambda&gt;\n  File &#34;&lt;command-567969701946348&gt;&#34;, line 32, in myTransEnrich\nNameError: name &#39;json&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:628)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:780)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:581)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$collectPartitions$1(PythonRDD.scala:197)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2379)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$8(Executor.scala:677)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:680)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2519)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2466)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2460)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2460)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1152)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2668)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2656)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2339)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2360)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2379)\n\tat org.apache.spark.api.python.PythonRDD$.collectPartitions(PythonRDD.scala:197)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:217)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;json&#39; is not defined&#39;, from &lt;command-567969701946348&gt;, line 32. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 676, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 668, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 279, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 1469, in takeUpToNumLeft\n    yield next(iterator)\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 110, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-567969701946348&gt;&#34;, line 56, in &lt;lambda&gt;\n  File &#34;&lt;command-567969701946348&gt;&#34;, line 32, in myTransEnrich\nNameError: name &#39;json&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:628)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:780)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:581)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$collectPartitions$1(PythonRDD.scala:197)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2379)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$8(Executor.scala:677)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:680)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>","errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 235.0 failed 1 times, most recent failure: Lost task 1.0 in stage 235.0 (TID 760, ip-10-172-247-115.us-west-2.compute.internal, executor driver): org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;json&#39; is not defined&#39;, from &lt;command-567969701946348&gt;, line 32. Full traceback below:","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-567969701946348&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     56</span> df_enriched <span class=\"ansi-blue-fg\">=</span> df_trans<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> row<span class=\"ansi-blue-fg\">:</span> myTransEnrich<span class=\"ansi-blue-fg\">(</span>row<span class=\"ansi-blue-fg\">,</span> bc_store<span class=\"ansi-blue-fg\">,</span> bc_items<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     57</span> print<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;df_enriched:0:&#34;</span><span class=\"ansi-blue-fg\">,</span> df_enriched<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">---&gt; 58</span><span class=\"ansi-red-fg\"> </span>df_enriched<span class=\"ansi-blue-fg\">.</span>toDF<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;TId&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;StoreId&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;ItemId&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;TDesc&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;TDate&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">toDF</span><span class=\"ansi-blue-fg\">(self, schema, sampleRatio)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-blue-fg\">[</span>Row<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;Alice&#39;</span><span class=\"ansi-blue-fg\">,</span> age<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     63</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">---&gt; 64</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> sparkSession<span class=\"ansi-blue-fg\">.</span>createDataFrame<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> sampleRatio<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     65</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     66</span>     RDD<span class=\"ansi-blue-fg\">.</span>toDF <span class=\"ansi-blue-fg\">=</span> toDF\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/databricks/utils/instrumentation.py</span> in <span class=\"ansi-cyan-fg\">wrapper</span><span class=\"ansi-blue-fg\">(self, *args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     40</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     41</span>             start_time <span class=\"ansi-blue-fg\">=</span> time<span class=\"ansi-blue-fg\">.</span>time<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">---&gt; 42</span><span class=\"ansi-red-fg\">             </span>return_val <span class=\"ansi-blue-fg\">=</span> func<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kwargs<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     43</span>         <span class=\"ansi-green-fg\">except</span> Exception<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     44</span>             duration <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">(</span>time<span class=\"ansi-blue-fg\">.</span>time<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">-</span> start_time<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">*</span> <span class=\"ansi-cyan-fg\">1000</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">createDataFrame</span><span class=\"ansi-blue-fg\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    656</span>             return super(SparkSession, self).createDataFrame(\n<span class=\"ansi-green-intense-fg ansi-bold\">    657</span>                 data, schema, samplingRatio, verifySchema)\n<span class=\"ansi-green-fg\">--&gt; 658</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_create_dataframe<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> verifySchema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    659</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    660</span>     <span class=\"ansi-green-fg\">def</span> _create_dataframe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> verifySchema<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_create_dataframe</span><span class=\"ansi-blue-fg\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    686</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    687</span>             <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> RDD<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 688</span><span class=\"ansi-red-fg\">                 </span>rdd<span class=\"ansi-blue-fg\">,</span> schema <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_createFromRDD<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span>prepare<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    689</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    690</span>                 rdd<span class=\"ansi-blue-fg\">,</span> schema <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_createFromLocal<span class=\"ansi-blue-fg\">(</span>map<span class=\"ansi-blue-fg\">(</span>prepare<span class=\"ansi-blue-fg\">,</span> data<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_createFromRDD</span><span class=\"ansi-blue-fg\">(self, rdd, schema, samplingRatio)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    429</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    430</span>         <span class=\"ansi-green-fg\">if</span> schema <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">None</span> <span class=\"ansi-green-fg\">or</span> isinstance<span class=\"ansi-blue-fg\">(</span>schema<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">(</span>list<span class=\"ansi-blue-fg\">,</span> tuple<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 431</span><span class=\"ansi-red-fg\">             </span>struct <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_inferSchema<span class=\"ansi-blue-fg\">(</span>rdd<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> names<span class=\"ansi-blue-fg\">=</span>schema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    432</span>             converter <span class=\"ansi-blue-fg\">=</span> _create_converter<span class=\"ansi-blue-fg\">(</span>struct<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    433</span>             rdd <span class=\"ansi-blue-fg\">=</span> rdd<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span>converter<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_inferSchema</span><span class=\"ansi-blue-fg\">(self, rdd, samplingRatio, names)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    400</span>         <span class=\"ansi-blue-fg\">:</span><span class=\"ansi-green-fg\">return</span><span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">:</span><span class=\"ansi-green-fg\">class</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-red-fg\">`</span>pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>types<span class=\"ansi-blue-fg\">.</span>StructType<span class=\"ansi-red-fg\">`</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    401</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 402</span><span class=\"ansi-red-fg\">         </span>first <span class=\"ansi-blue-fg\">=</span> rdd<span class=\"ansi-blue-fg\">.</span>first<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    403</span>         <span class=\"ansi-green-fg\">if</span> <span class=\"ansi-green-fg\">not</span> first<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    404</span>             raise ValueError(&#34;The first row in RDD is empty, &#34;\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">first</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1491</span>         ValueError<span class=\"ansi-blue-fg\">:</span> RDD <span class=\"ansi-green-fg\">is</span> empty\n<span class=\"ansi-green-intense-fg ansi-bold\">   1492</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">-&gt; 1493</span><span class=\"ansi-red-fg\">         </span>rs <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>take<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1494</span>         <span class=\"ansi-green-fg\">if</span> rs<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1495</span>             <span class=\"ansi-green-fg\">return</span> rs<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">take</span><span class=\"ansi-blue-fg\">(self, num)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1473</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1474</span>             p <span class=\"ansi-blue-fg\">=</span> range<span class=\"ansi-blue-fg\">(</span>partsScanned<span class=\"ansi-blue-fg\">,</span> min<span class=\"ansi-blue-fg\">(</span>partsScanned <span class=\"ansi-blue-fg\">+</span> numPartsToTry<span class=\"ansi-blue-fg\">,</span> totalParts<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1475</span><span class=\"ansi-red-fg\">             </span>res <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">.</span>runJob<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> takeUpToNumLeft<span class=\"ansi-blue-fg\">,</span> p<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1476</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1477</span>             items <span class=\"ansi-blue-fg\">+=</span> res\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/context.py</span> in <span class=\"ansi-cyan-fg\">runJob</span><span class=\"ansi-blue-fg\">(self, rdd, partitionFunc, partitions, allowLocal)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1227</span>             <span class=\"ansi-green-fg\">finally</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1228</span>                 os<span class=\"ansi-blue-fg\">.</span>remove<span class=\"ansi-blue-fg\">(</span>filename<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1229</span><span class=\"ansi-red-fg\">         </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>runJob<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jsc<span class=\"ansi-blue-fg\">.</span>sc<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> mappedRDD<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">,</span> partitions<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1230</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> mappedRDD<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1231</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1304</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1305</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1307</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    126</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 127</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    128</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    129</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 235.0 failed 1 times, most recent failure: Lost task 1.0 in stage 235.0 (TID 760, ip-10-172-247-115.us-west-2.compute.internal, executor driver): org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;json&#39; is not defined&#39;, from &lt;command-567969701946348&gt;, line 32. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 676, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 668, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 279, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 1469, in takeUpToNumLeft\n    yield next(iterator)\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 110, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-567969701946348&gt;&#34;, line 56, in &lt;lambda&gt;\n  File &#34;&lt;command-567969701946348&gt;&#34;, line 32, in myTransEnrich\nNameError: name &#39;json&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:628)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:780)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:581)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$collectPartitions$1(PythonRDD.scala:197)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2379)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$8(Executor.scala:677)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:680)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2519)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2466)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2460)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2460)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1152)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2668)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2656)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2339)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2360)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2379)\n\tat org.apache.spark.api.python.PythonRDD$.collectPartitions(PythonRDD.scala:197)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:217)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;json&#39; is not defined&#39;, from &lt;command-567969701946348&gt;, line 32. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 676, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 668, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 279, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 1469, in takeUpToNumLeft\n    yield next(iterator)\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 110, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-567969701946348&gt;&#34;, line 56, in &lt;lambda&gt;\n  File &#34;&lt;command-567969701946348&gt;&#34;, line 32, in myTransEnrich\nNameError: name &#39;json&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:628)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:780)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:581)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$collectPartitions$1(PythonRDD.scala:197)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2379)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$8(Executor.scala:677)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:680)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark.sql.functions as f\nfrom pyspark.sql.types import StructType,StructField, StringType, LongType\n\nprint(\"================== DataFrame.JOIN ==================\")\n\ncolumns = [\"Id\", \"Name\", \"City\", \"State\", \"Country\", \"Active\"]\ndata = [\n  (1, \"Store-A\", \"New York\", \"New York\", \"USA\", 1),\n  (2, \"Store-B\", \"Washington\", \"Seattle\", \"USA\", 1),\n  (3, \"Store-C\", \"Dallas\", \"Texas\", \"USA\", 1)\n]\ndf_store = spark.createDataFrame(data= data, schema = columns )\n\ncolumns = [\"Id\", \"Name\", \"Desc\", \"Active\"]\ndata = [(1, \"Bananas\", \"Bananas\", \"1\"),\n             (2,\"Apples\",\"Gala Apples\",\"1\"),\n             (3, \"Organes\", \"Naval Oranges\", \"1\")]\n\ndf_items = spark.createDataFrame(data= data, schema = columns )\n\ncolumns = [\"TId\", \"StoreId\", \"ItemId\", \"TDesc\", \"TDate\"]\ndata = [(1, 1, 2, \"Apples for StoreB@WAS\", \"2021-04-19T01:01:01\"),\n             (1, 3, 3, \"Oranges for StoreC@DAL\", \"2021-04-19T02:01:01\"),\n             (1, 2, 1, \"Bananas for StoreA@NYC\", \"2021-04-19T03:01:01\")]\ndf_trans = spark.createDataFrame(data= data, schema = columns )\n\n#using dataframe joins\ncolumns = [\"TId\", \"TDesc\", \"TDate\", \"Name\", \"StoreId\"]\n#Trans: [\"TId\", \"StoreId\", \"ItemId\", \"TDesc\", \"TDate\"]\n#Items: [\"Id\", \"Name\", \"Desc\", \"Active\"]\ndf_trans_items = df_trans.join(df_items, df_items.Id == df_trans.ItemId, \"inner\").select([col for col in columns])\ndf_trans_items = df_trans_items.withColumnRenamed(\"Name\", \"ItemName\")\nprint(\"df_trans_items:\", df_trans_items)\n\ncolumns = [\"TId\", \"TDesc\", \"TDate\", \"ItemName\", \"Name\", \"City\", \"State\"]\n#Trans:        [\"TId\", \"StoreId\", \"ItemId\", \"TDesc\", \"TDate\"]\n#Trans+Items:  [\"TId\", \"ItemName\", \"TDesc\", \"TDate\"]\n#Store:        [\"Id\", \"Name\", \"City\", \"State\", \"Country\", \"Active\"]\ndf_trans_items_stores = df_trans_items.join(df_store, df_store.Id == df_trans_items.StoreId, \"inner\").select([col for col in columns])\ndf_trans_items_stores = df_trans_items_stores.withColumnRenamed(\"Name\", \"StoreName\")\ndf_trans_items_stores = df_trans_items_stores.withColumn(\"Location\", f.concat(df_trans_items_stores[\"City\"],df_trans_items_stores[\"State\"]))\nprint(\"df_trans_items_stores:\", df_trans_items_stores)\n\ndf_trans_items_stores.show()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f2168ea7-bd5e-477c-ba68-b29dd5c6073a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">================== DataFrame.JOIN ==================\ndf_trans_items: DataFrame[TId: bigint, TDesc: string, TDate: string, ItemName: string, StoreId: bigint]\ndf_trans_items_stores: DataFrame[TId: bigint, TDesc: string, TDate: string, ItemName: string, StoreName: string, City: string, State: string, Location: string]\n+---+--------------------+-------------------+--------+---------+----------+--------+-----------------+\n|TId|               TDesc|              TDate|ItemName|StoreName|      City|   State|         Location|\n+---+--------------------+-------------------+--------+---------+----------+--------+-----------------+\n|  1|Apples for StoreB...|2021-04-19T01:01:01|  Apples|  Store-A|  New York|New York| New YorkNew York|\n|  1|Bananas for Store...|2021-04-19T03:01:01| Bananas|  Store-B|Washington| Seattle|WashingtonSeattle|\n|  1|Oranges for Store...|2021-04-19T02:01:01| Organes|  Store-C|    Dallas|   Texas|      DallasTexas|\n+---+--------------------+-------------------+--------+---------+----------+--------+-----------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">================== DataFrame.JOIN ==================\ndf_trans_items: DataFrame[TId: bigint, TDesc: string, TDate: string, ItemName: string, StoreId: bigint]\ndf_trans_items_stores: DataFrame[TId: bigint, TDesc: string, TDate: string, ItemName: string, StoreName: string, City: string, State: string, Location: string]\n+---+--------------------+-------------------+--------+---------+----------+--------+-----------------+\nTId|               TDesc|              TDate|ItemName|StoreName|      City|   State|         Location|\n+---+--------------------+-------------------+--------+---------+----------+--------+-----------------+\n  1|Apples for StoreB...|2021-04-19T01:01:01|  Apples|  Store-A|  New York|New York| New YorkNew York|\n  1|Bananas for Store...|2021-04-19T03:01:01| Bananas|  Store-B|Washington| Seattle|WashingtonSeattle|\n  1|Oranges for Store...|2021-04-19T02:01:01| Organes|  Store-C|    Dallas|   Texas|      DallasTexas|\n+---+--------------------+-------------------+--------+---------+----------+--------+-----------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#SQL Style\ncolumns = [\"Id\", \"Name\", \"City\", \"State\", \"Country\", \"Active\"]\ndata = [\n  (1, \"Store-A\", \"New York\", \"New York\", \"USA\", 1),\n  (2, \"Store-B\", \"Washington\", \"Seattle\", \"USA\", 1),\n  (3, \"Store-C\", \"Dallas\", \"Texas\", \"USA\", 1)\n]\ndf_store = spark.createDataFrame(data= data, schema = columns )\nprint(\"df_store\")\ndf_store.show()\ndf_store.createOrReplaceTempView(\"stores\")\n\ncolumns = [\"Id\", \"Name\", \"Desc\", \"Active\"]\ndata = [(1, \"Bananas\", \"Bananas\", \"1\"),\n             (2,\"Apples\",\"Gala Apples\",\"1\"),\n             (3, \"Organes\", \"Naval Oranges\", \"1\")]\n\ndf_items = spark.createDataFrame(data= data, schema = columns )\nprint(\"df_items\")\ndf_items.show()\ndf_items.createOrReplaceTempView(\"items\")\n\n  \ncolumns = [\"TId\", \"StoreId\", \"ItemId\", \"TDesc\", \"TDate\"]\ndata = [(1, 1, 2, \"Apples for StoreB@WAS\", \"2021-04-19T01:01:01\"),\n             (1, 3, 3, \"Oranges for StoreC@DAL\", \"2021-04-19T02:01:01\"),\n             (1, 2, 1, \"Bananas for StoreA@NYC\", \"2021-04-19T03:01:01\")]\ndf_trans = spark.createDataFrame(data= data, schema = columns )\ndf_trans.createOrReplaceTempView(\"transactions\")\nprint(\"df_trans\")\ndf_trans.show()\n\nenrich_df = spark.sql(\"select t.TId, s.Name as Store_Name, i.Name as Item_name, t.TDesc, t.TDate, s.Active as Store_Active, i.Active as Item_Active, s.City from items i, stores s, transactions t where t.ItemId = i.Id and t.StoreId = s.Id\")\nprint(\"enrich_df::\", enrich_df)\nenrich_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc4e1844-3eea-47b4-9345-404232f3921c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark.sql.functions as f\nfrom pyspark.sql.types import StructType,StructField, StringType, LongType\n\n#Using WithColumn & UDF\nprint(\"================== DataFrame.WithColumn ==================\")\nprint(\"df_trans:\", df_trans)\n\ndef getItemValues(itemId):\n  print(\"getItemValues:\", type(itemId), itemId)\n  #getItemValues: <class 'pyspark.sql.column.Column'> Column<b'ItemId'>\n  #['{\"Id\":1,\"Name\":\"Bananas\",\"Desc\":\"Bananas\",\"Active\":\"1\"}']\n  \n  return itemId\n\nbc_items = spark.sparkContext.broadcast(df_items.toJSON().collect())        \ngetItemValues = udf(getItemValues, StringType())\ndf_new = df_trans.withColumn(\"ItemName\", getItemValues(df_trans.ItemId)) #f.lit(\"ITEM_NAME\")\ndf_new = df_new.drop(\"ItemId\")\n\ndef getStoreValues(storeId):\n  print(\"getStoreValues:\", type(storeId), storeId)\n  #getStoreValues: <class 'pyspark.sql.column.Column'> Column<b'StoreId'>\n  return storeId\n\ndf_store = spark.sparkContext.broadcast(df_store.toJSON().collect())\ngetStoreValues = udf(getStoreValues, StringType())\ndf_new = df_new.withColumn(\"StoreName\", f.lit(\"SITE_NAME\")) #getStoreValues(df_trans.StoreId, df_store)\ndf_new = df_new.drop(\"StoreId\")\n\nprint(\"df_enriched:2:\", df_new)\ndf_new.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5493756a-7771-4600-9253-9d9a8e8914e5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">================== DataFrame.WithColumn ==================\ndf_trans: DataFrame[TId: bigint, StoreId: bigint, ItemId: bigint, TDesc: string, TDate: string]\ndf_enriched:2: DataFrame[TId: bigint, TDesc: string, TDate: string, ItemName: string, StoreName: string]\n+---+--------------------+-------------------+--------+---------+\n|TId|               TDesc|              TDate|ItemName|StoreName|\n+---+--------------------+-------------------+--------+---------+\n|  1|Apples for StoreB...|2021-04-19T01:01:01|       2|SITE_NAME|\n|  1|Oranges for Store...|2021-04-19T02:01:01|       3|SITE_NAME|\n|  1|Bananas for Store...|2021-04-19T03:01:01|       1|SITE_NAME|\n+---+--------------------+-------------------+--------+---------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">================== DataFrame.WithColumn ==================\ndf_trans: DataFrame[TId: bigint, StoreId: bigint, ItemId: bigint, TDesc: string, TDate: string]\ndf_enriched:2: DataFrame[TId: bigint, TDesc: string, TDate: string, ItemName: string, StoreName: string]\n+---+--------------------+-------------------+--------+---------+\nTId|               TDesc|              TDate|ItemName|StoreName|\n+---+--------------------+-------------------+--------+---------+\n  1|Apples for StoreB...|2021-04-19T01:01:01|       2|SITE_NAME|\n  1|Oranges for Store...|2021-04-19T02:01:01|       3|SITE_NAME|\n  1|Bananas for Store...|2021-04-19T03:01:01|       1|SITE_NAME|\n+---+--------------------+-------------------+--------+---------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cab177c6-e491-4847-8ac8-8aed4f7ed475"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-functions-enrich","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3119029963728517}},"nbformat":4,"nbformat_minor":0}
