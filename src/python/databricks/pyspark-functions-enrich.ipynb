{"cells":[{"cell_type":"code","source":["pip install json5"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27ec5b96-7fcd-4be3-bc66-9bdf4d002925"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting json5\n  Downloading json5-0.9.5-py2.py3-none-any.whl (17 kB)\nInstalling collected packages: json5\nSuccessfully installed json5-0.9.5\nPython interpreter will be restarted.\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting json5\n  Downloading json5-0.9.5-py2.py3-none-any.whl (17 kB)\nInstalling collected packages: json5\nSuccessfully installed json5-0.9.5\nPython interpreter will be restarted.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#Use case\n#Community Master Data: Users, Roles, UserRole\n#Transaction Data: Transactions (transId, userId, some details)\n#From userId, get the relevant username and perform anyother lookups\n\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col,sum,avg\nfrom pyspark.sql.types import Row\nimport json5 as json\n\ncolumns = [\"RId\",\"RName\", \"RActive\"]\ndata = [(1, \"Role1\", 1),\n    (2, \"Role2\", 1),\n    (3, \"Role3\", 1)]\ndf_role = spark.createDataFrame(data=data, schema=columns)\ndf_role.createOrReplaceTempView(\"roles\")\nbc_role = spark.sparkContext.broadcast(df_role.toJSON().collect())\n\ncolumns = [\"UId\",\"UName\", \"UActive\"]\ndata = [(1, \"User1\", 1),\n    (2, \"User2\", 1),\n    (3, \"User3\", 0),\n    (4, \"User4\", 1)]\ndf_user = spark.createDataFrame(data=data, schema=columns)\ndf_user.createOrReplaceTempView(\"users\")\nbc_user = spark.sparkContext.broadcast(df_user.toJSON().collect())\n\ncolumns = [\"UId\", \"RId\"]\ndata = [(1, 1), (1, 2), (2, 3), (3, 1), (3,2), (3,3), (4, 1), (4,3)]\ndf_user_role = spark.createDataFrame(data=data, schema=columns)\ndf_user_role.createOrReplaceTempView(\"user_role\")\nbc_user_role = spark.sparkContext.broadcast(df_user_role.toJSON().collect())\n\n\ndef myMapFunction(rec, bc_user, bc_role, bc_user_role):\n  print(\"myMapFunction::Type:%s, Data:%s\" % (type(rec), rec))\n  #myMapFunction::Type:<class 'pyspark.sql.types.Row'>, Data:Row(TId=4, UId=3, TDesc='TDesc4', TDate='2021-04-16T17:14:00')\n  row_dict = rec.asDict()\n  userId = row_dict.get(\"UId\")\n  print(\"myMapFunction::user:%s, bc_user:%s, bc_user.value:%s\" % (userId, type(bc_user), type(bc_user.value)))\n  enrich1_val = \"\"\n  for v in bc_user.value:\n    print(\"myMapFunction::type(v):%s, v:%s\" % (type(v), v))\n    #myMapFunction::type(v):<class 'str'>, v:{\"UId\":1,\"UName\":\"U1\",\"UActive\":1}\n    if type(v) == str:\n      v = json.loads(v)\n    if v.get(\"UId\") == userId:\n      enrich1_val = v.get(\"UName\")\n  #myMapFunction::user:3, bc_user:<class 'pyspark.broadcast.Broadcast'>, bc_user.value:<class 'list'>\n  row_dict[\"ENRICH_1\"] = enrich1_val #Adding a new column and setting some value.\n  print(\"myMapFunction::row_dict:%s\" % row_dict)\n  #myMapFunction::row_dict:{'TId': 4, 'UId': 3, 'TDesc': 'TDesc4', 'TDate': '2021-04-16T17:14:00', 'ENRICH_1': 'constant_value'}\n  return Row(**row_dict)\n\ncolumns = [\"TId\", \"UId\", \"TDesc\", \"TDate\"]\ndata = [\n  (1, 1, \"TDesc1\", \"2021-04-16T17:11:00\"), \n  (2, 1, \"TDesc2\", \"2021-04-16T17:12:00\"), \n  (3, 4, \"TDesc3\", \"2021-04-16T17:13:00\"), \n  (4, 3, \"TDesc4\", \"2021-04-16T17:14:00\"), \n  (5, 2, \"TDesc5\", \"2021-04-16T17:15:00\")]\ndf_trans = spark.createDataFrame(data=data, schema=columns)\n\ndf_new_rdd = df_trans.rdd.map(lambda r: myMapFunction(r, bc_user, bc_role, bc_user_role))\nprint(\"df_new_rdd:\", df_new_rdd)\ndf_new_rdd.toDF([\"TId\", \"UId\", \"TDesc\", \"TDate\", \"ENRICH_1\"]).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e9ba1dd9-41cc-41c1-bfba-e9f6a6c09423"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">df_new_rdd: PythonRDD[1078] at RDD at PythonRDD.scala:58\n+---+---+------+-------------------+--------+\n|TId|UId| TDesc|              TDate|ENRICH_1|\n+---+---+------+-------------------+--------+\n|  1|  1|TDesc1|2021-04-16T17:11:00|   User1|\n|  2|  1|TDesc2|2021-04-16T17:12:00|   User1|\n|  3|  4|TDesc3|2021-04-16T17:13:00|   User4|\n|  4|  3|TDesc4|2021-04-16T17:14:00|   User3|\n|  5|  2|TDesc5|2021-04-16T17:15:00|   User2|\n+---+---+------+-------------------+--------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">df_new_rdd: PythonRDD[1078] at RDD at PythonRDD.scala:58\n+---+---+------+-------------------+--------+\nTId|UId| TDesc|              TDate|ENRICH_1|\n+---+---+------+-------------------+--------+\n  1|  1|TDesc1|2021-04-16T17:11:00|   User1|\n  2|  1|TDesc2|2021-04-16T17:12:00|   User1|\n  3|  4|TDesc3|2021-04-16T17:13:00|   User4|\n  4|  3|TDesc4|2021-04-16T17:14:00|   User3|\n  5|  2|TDesc5|2021-04-16T17:15:00|   User2|\n+---+---+------+-------------------+--------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"836bad72-c31d-4a38-a55d-60de0880206a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-functions-enrich","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3119029963728517}},"nbformat":4,"nbformat_minor":0}
