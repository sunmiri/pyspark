{"cells":[{"cell_type":"code","source":["# #### JOIN ####\ncolumns = [\"RId\",\"RName\", \"RActive\"]\ndata = [(1, \"R1\", 1),\n    (2, \"R2\", 1),\n    (3, \"R3\", 1)]\ndf_role = spark.createDataFrame(data=data, schema=columns)\ndf_role.createOrReplaceTempView(\"roles\")\n\ncolumns = [\"UId\",\"UName\", \"UActive\"]\ndata = [(1, \"U1\", 1),\n    (2, \"U2\", 1),\n    (3, \"U3\", 0),\n    (4, \"U4\", 1)]\ndf_user = spark.createDataFrame(data=data, schema=columns)\ndf_user.createOrReplaceTempView(\"users\")\n\ndf_role.show()\n\ndf_user.show()\n\ncolumns = [\"UId\", \"RId\"]\ndata = [(1, 1), (1, 2), (2, 3), (3, 1), (3,2), (3,3), (4, 1), (4,3)]\ndf_user_role = spark.createDataFrame(data=data, schema=columns)\ndf_user_role.createOrReplaceTempView(\"user_role\")\n\nprint(\"Spark-Sql Approach\")\nnew_df = spark.sql(\"select u.UName, r.RName, u.UActive, r.RActive from users as u, roles as r, user_role as ur where ur.UId = u.UId and ur.RId = r.RId and UActive=1\")\nnew_df.show()\n\nprint(\"Dataframe-Join Approach\")\nprint(\"df_user_role.join(df_user)\")\ndf1 = df_user_role.join(df_user, df_user_role.UId == df_user.UId, \"inner\")\ndf1.show()\n\nprint(\"user_role_user.join(df_role)\")\ndf2 = df1.join(df_role, df_role.RId == df1.RId, \"inner\")\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f93ea5c-350d-4886-be45-eeef16192bd4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#SQL and DF\ncolumns = [\"UId\",\"Name\", \"Zone\"]\ndata = [(\"1\", \"User1\", \"Z1\"),\n    (\"2\", \"User2\", \"Z1\"),\n    (\"3\", \"User3\", \"Z2\")]\n\ndf = spark.createDataFrame(data=data, schema=columns)\n\n#Spark DF based\ndf.show(truncate=False)\ndf.groupBy(\"Zone\").count().show(truncate=False)\n\n#Spark-Sql Based\ndf.createOrReplaceTempView(\"user_zone\")\ndf_sql = spark.sql(\"SELECT Zone, count(Name) FROM user_zone group by Zone\")\ndf_sql.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"074bff74-f14a-4f6b-a01f-f9c30a693eb7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#UDF - Single Row/Record\n#UDFâ€™s a.k.a User Defined Functions\n#extend and reuse function in sql\n\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col,sum,avg\n\ndef uppercase(str):\n    return str.upper() \n  \nconvertUDF = udf(lambda z: uppercase(z)) \n\ndf.select(convertUDF(col(\"Name\")).alias(\"Name\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"006444f3-1a3a-4711-833c-bc66f61a5cff"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#Aggregations\nimport pyspark\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import types as T\n\ncolumns = [\"UId\",\"Name\", \"Zone\"]\ndata = [(\"1\", \"User1\", \"Z1\"),\n    (\"2\", \"User2\", \"Z1\"),\n    (\"3\", \"User3\", \"Z2\")]\n\ndf = spark.createDataFrame(data=data, schema=columns)\n\ndf.groupBy('Zone').agg(F.collect_list('Name').alias('value_list')).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb820c51-a0c3-4745-8487-14fd8822fde1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----+--------------+\n|Zone|    value_list|\n+----+--------------+\n|  Z1|[User1, User2]|\n|  Z2|       [User3]|\n+----+--------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+--------------+\nZone|    value_list|\n+----+--------------+\n  Z1|[User1, User2]|\n  Z2|       [User3]|\n+----+--------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#UDF Aggregation\nimport pyspark\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import types as T\n\ncolumns = [\"UId\",\"Name\", \"Zone\"]\ndata = [(\"1\", \"User1\", \"Z1\"),\n    (\"2\", \"User2\", \"Z1\"),\n    (\"3\", \"User3\", \"Z2\")]\n\ndf = spark.createDataFrame(data=data, schema=columns)\n\ndef concat(x):\n  print(\"concat:\", type(x), x)\n  result = \"\"\n  for i in x:\n    result = result + \"|\" + i\n  return result\n\n#Function and Return Type\nconcat_udf = F.udf(concat, T.StringType())\ndf.groupBy('Zone').agg(concat_udf(F.collect_list('Name')).alias('users_in_zone')).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1ba5f25-dd12-4049-83e6-296fc5232679"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----+-------------+\n|Zone|users_in_zone|\n+----+-------------+\n|  Z1| |User1|User2|\n|  Z2|       |User3|\n+----+-------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-------------+\nZone|users_in_zone|\n+----+-------------+\n  Z1| |User1|User2|\n  Z2|       |User3|\n+----+-------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53ae9211-bed6-4d5b-86b5-94dbe6ca8834"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-joins-udf","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3119029963728512}},"nbformat":4,"nbformat_minor":0}
