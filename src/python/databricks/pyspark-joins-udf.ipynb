{"cells":[{"cell_type":"code","source":["# #### JOIN ####\ncolumns = [\"RId\",\"RName\", \"RActive\"]\ndata = [(1, \"R1\", 1),\n    (2, \"R2\", 1),\n    (3, \"R3\", 1)]\ndf_role = spark.createDataFrame(data=data, schema=columns)\ndf_role.createOrReplaceTempView(\"roles\")\n\ncolumns = [\"UId\",\"UName\", \"UActive\"]\ndata = [(1, \"U1\", 1),\n    (2, \"U2\", 1),\n    (3, \"U3\", 0),\n    (4, \"U4\", 1)]\ndf_user = spark.createDataFrame(data=data, schema=columns)\ndf_user.createOrReplaceTempView(\"users\")\n\ndf_role.show()\n\ndf_user.show()\n\ncolumns = [\"UId\", \"RId\"]\ndata = [(1, 1), (1, 2), (2, 3), (3, 1), (3,2), (3,3), (4, 1), (4,3)]\ndf_user_role = spark.createDataFrame(data=data, schema=columns)\ndf_user_role.createOrReplaceTempView(\"user_role\")\n\nprint(\"Spark-Sql Approach\")\nnew_df = spark.sql(\"select u.UName, r.RName, u.UActive, r.RActive from users as u, roles as r, user_role as ur where ur.UId = u.UId and ur.RId = r.RId and UActive=1\")\nnew_df.show()\n\nprint(\"Dataframe-Join Approach\")\nprint(\"df_user_role.join(df_user)\")\ndf1 = df_user_role.join(df_user, df_user_role.UId == df_user.UId, \"inner\")\ndf1.show()\n\nprint(\"user_role_user.join(df_role)\")\ndf2 = df1.join(df_role, df_role.RId == df1.RId, \"inner\")\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f93ea5c-350d-4886-be45-eeef16192bd4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#SQL and DF\ncolumns = [\"UId\",\"Name\", \"Zone\"]\ndata = [(\"1\", \"User1\", \"Z1\"),\n    (\"2\", \"User2\", \"Z1\"),\n    (\"3\", \"User3\", \"Z2\")]\n\ndf = spark.createDataFrame(data=data, schema=columns)\n\n#Spark DF based\ndf.show(truncate=False)\ndf.groupBy(\"Zone\").count().show(truncate=False)\n\n#Spark-Sql Based\ndf.createOrReplaceTempView(\"user_zone\")\ndf_sql = spark.sql(\"SELECT Zone, count(Name) FROM user_zone group by Zone\")\ndf_sql.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"074bff74-f14a-4f6b-a01f-f9c30a693eb7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#UDF - Single Row/Record\n#UDFâ€™s a.k.a User Defined Functions\n#extend and reuse function in sql\n\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col,sum,avg\n\ndef uppercase(str):\n    return str.upper() \n  \nconvertUDF = udf(lambda z: uppercase(z)) \n\ndf.select(convertUDF(col(\"Name\")).alias(\"Name\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"006444f3-1a3a-4711-833c-bc66f61a5cff"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Aggregations\nimport pyspark\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import types as T\n\ncolumns = [\"UId\",\"Name\", \"Zone\"]\ndata = [(\"1\", \"User1\", \"Z1\"),\n    (\"2\", \"User2\", \"Z1\"),\n    (\"3\", \"User3\", \"Z2\")]\n\ndf = spark.createDataFrame(data=data, schema=columns)\n\ndf.groupBy('Zone').agg(F.collect_list('Name').alias('value_list')).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb820c51-a0c3-4745-8487-14fd8822fde1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----+--------------+\n|Zone|    value_list|\n+----+--------------+\n|  Z1|[User1, User2]|\n|  Z2|       [User3]|\n+----+--------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+--------------+\nZone|    value_list|\n+----+--------------+\n  Z1|[User1, User2]|\n  Z2|       [User3]|\n+----+--------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#UDF Aggregation\nimport pyspark\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import types as T\n\ncolumns = [\"UId\",\"Name\", \"Zone\"]\ndata = [(\"1\", \"User1\", \"Z1\"),\n    (\"2\", \"User2\", \"Z1\"),\n    (\"3\", \"User3\", \"Z2\")]\n\ndf = spark.createDataFrame(data=data, schema=columns)\n\ndef concat(x):\n  print(\"concat:\", type(x), x)\n  result = \"\"\n  for i in x:\n    result = result + \"|\" + i\n  return result\n\n#Function and Return Type\nconcat_udf = F.udf(concat, T.StringType())\ndf.groupBy('Zone').agg(concat_udf(F.collect_list('Name')).alias('users_in_zone')).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1ba5f25-dd12-4049-83e6-296fc5232679"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----+-------------+\n|Zone|users_in_zone|\n+----+-------------+\n|  Z1| |User1|User2|\n|  Z2|       |User3|\n+----+-------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-------------+\nZone|users_in_zone|\n+----+-------------+\n  Z1| |User1|User2|\n  Z2|       |User3|\n+----+-------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col,sum,avg\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import types as T\nimport time\nimport datetime\n\ncolumns = [\"date\", \"delay\", \"distance\", \"origin\", \"destination\"]\ndata = [(1619124792403, 1, 50, \"SFO\", \"SJC\"),\n    (1619134792403, 1, 250, \"SJO\", \"LAX\"),\n    (1619144792403, 1, 60, \"LAX\", \"SDG\")]\n\ndf = spark.createDataFrame(data=data, schema=columns)\nprint(\"df:\", df)\ndf.show()\ndf.createOrReplaceTempView(\"distances\")\n\ndef parseDate(v):\n  print(\"parseDate::type(v):\", type(v), \", val:\", v)\n  dt = datetime.datetime.fromtimestamp(v/1000.0, tz=datetime.timezone.utc)\n  #parseDate::type(v): <class 'int'> , val: 1\n  return dt\n  \nparsedate_udf = F.udf(parseDate, T.TimestampType())\nspark.udf.register(\"parsedate_udf\", lambda v: parseDate(v) if not v is None else -1 , T.TimestampType())\n#1\nspark.sql(\"select delay, parsedate_udf(date), distance, origin, destination  from distances\").show()\n#2 - since date transformation has predefined function\nspark.sql(\"select delay, date_format('date', 'MM/dd/yyy'), distance, origin, destination  from distances\").show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53ae9211-bed6-4d5b-86b5-94dbe6ca8834"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">df: DataFrame[date: bigint, delay: bigint, distance: bigint, origin: string, destination: string]\n+-------------+-----+--------+------+-----------+\n|         date|delay|distance|origin|destination|\n+-------------+-----+--------+------+-----------+\n|1619124792403|    1|      50|   SFO|        SJC|\n|1619134792403|    1|     250|   SJO|        LAX|\n|1619144792403|    1|      60|   LAX|        SDG|\n+-------------+-----+--------+------+-----------+\n\n+-----+--------------------+--------+------+-----------+\n|delay| parsedate_udf(date)|distance|origin|destination|\n+-----+--------------------+--------+------+-----------+\n|    1|2021-04-22 20:53:...|      50|   SFO|        SJC|\n|    1|2021-04-22 23:39:...|     250|   SJO|        LAX|\n|    1|2021-04-23 02:26:...|      60|   LAX|        SDG|\n+-----+--------------------+--------+------+-----------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">df: DataFrame[date: bigint, delay: bigint, distance: bigint, origin: string, destination: string]\n+-------------+-----+--------+------+-----------+\n         date|delay|distance|origin|destination|\n+-------------+-----+--------+------+-----------+\n1619124792403|    1|      50|   SFO|        SJC|\n1619134792403|    1|     250|   SJO|        LAX|\n1619144792403|    1|      60|   LAX|        SDG|\n+-------------+-----+--------+------+-----------+\n\n+-----+--------------------+--------+------+-----------+\ndelay| parsedate_udf(date)|distance|origin|destination|\n+-----+--------------------+--------+------+-----------+\n    1|2021-04-22 20:53:...|      50|   SFO|        SJC|\n    1|2021-04-22 23:39:...|     250|   SJO|        LAX|\n    1|2021-04-23 02:26:...|      60|   LAX|        SDG|\n+-----+--------------------+--------+------+-----------+\n\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3119029963728522&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     28</span> spark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;select delay, parsedate_udf(date), distance, origin, destination  from distances&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     29</span> <span class=\"ansi-red-fg\">#2 - since date transformation has predefined function</span>\n<span class=\"ansi-green-fg\">---&gt; 30</span><span class=\"ansi-red-fg\"> </span>spark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;select delay, date_format(date, &#39;MM/dd/yyy&#39;), distance, origin, destination  from distances&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">sql</span><span class=\"ansi-blue-fg\">(self, sqlQuery)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    707</span>         <span class=\"ansi-blue-fg\">[</span>Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row1&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row2&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">3</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row3&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    708</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 709</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jsparkSession<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span>sqlQuery<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_wrapped<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    710</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    711</span>     <span class=\"ansi-blue-fg\">@</span>since<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">2.0</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1304</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1305</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1307</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    131</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    132</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n<span class=\"ansi-green-fg\">--&gt; 133</span><span class=\"ansi-red-fg\">                 </span>raise_from<span class=\"ansi-blue-fg\">(</span>converted<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    134</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    135</span>                 <span class=\"ansi-green-fg\">raise</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">raise_from</span><span class=\"ansi-blue-fg\">(e)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: cannot resolve &#39;date_format(distances.`date`, &#39;MM/dd/yyy&#39;)&#39; due to data type mismatch: argument 1 requires timestamp type, however, &#39;distances.`date`&#39; is of bigint type.; line 1 pos 14;\n&#39;Project [delay#592L, unresolvedalias(date_format(date#591L, MM/dd/yyy, Some(Etc/UTC)), None), distance#593L, origin#594, destination#595]\n+- SubqueryAlias distances\n   +- LogicalRDD [date#591L, delay#592L, distance#593L, origin#594, destination#595], false\n</div>","errorSummary":"<span class=\"ansi-red-fg\">AnalysisException</span>: cannot resolve &#39;date_format(distances.`date`, &#39;MM/dd/yyy&#39;)&#39; due to data type mismatch: argument 1 requires timestamp type, however, &#39;distances.`date`&#39; is of bigint type.; line 1 pos 14;","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3119029963728522&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     28</span> spark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;select delay, parsedate_udf(date), distance, origin, destination  from distances&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     29</span> <span class=\"ansi-red-fg\">#2 - since date transformation has predefined function</span>\n<span class=\"ansi-green-fg\">---&gt; 30</span><span class=\"ansi-red-fg\"> </span>spark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;select delay, date_format(date, &#39;MM/dd/yyy&#39;), distance, origin, destination  from distances&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">sql</span><span class=\"ansi-blue-fg\">(self, sqlQuery)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    707</span>         <span class=\"ansi-blue-fg\">[</span>Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row1&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row2&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">3</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row3&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    708</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 709</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jsparkSession<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span>sqlQuery<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_wrapped<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    710</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    711</span>     <span class=\"ansi-blue-fg\">@</span>since<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">2.0</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1304</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1305</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1307</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    131</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    132</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n<span class=\"ansi-green-fg\">--&gt; 133</span><span class=\"ansi-red-fg\">                 </span>raise_from<span class=\"ansi-blue-fg\">(</span>converted<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    134</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    135</span>                 <span class=\"ansi-green-fg\">raise</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">raise_from</span><span class=\"ansi-blue-fg\">(e)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: cannot resolve &#39;date_format(distances.`date`, &#39;MM/dd/yyy&#39;)&#39; due to data type mismatch: argument 1 requires timestamp type, however, &#39;distances.`date`&#39; is of bigint type.; line 1 pos 14;\n&#39;Project [delay#592L, unresolvedalias(date_format(date#591L, MM/dd/yyy, Some(Etc/UTC)), None), distance#593L, origin#594, destination#595]\n+- SubqueryAlias distances\n   +- LogicalRDD [date#591L, delay#592L, distance#593L, origin#594, destination#595], false\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e68cacec-1c6a-41a0-bf16-4dd69a6f5133"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-joins-udf","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3119029963728512}},"nbformat":4,"nbformat_minor":0}
