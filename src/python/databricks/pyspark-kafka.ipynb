{"cells":[{"cell_type":"code","source":["pip install pyspark jproperties argparse json5"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1993c06b-18ea-4ba3-a4ea-7f9dd4aa5186"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting argparse\n  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\nCollecting jproperties\n  Downloading jproperties-2.1.0-py3-none-any.whl (17 kB)\nRequirement already satisfied: six~=1.12 in /databricks/python3/lib/python3.7/site-packages (from jproperties) (1.14.0)\nCollecting json5\n  Downloading json5-0.9.5-py2.py3-none-any.whl (17 kB)\nCollecting pyspark\n  Downloading pyspark-3.0.1.tar.gz (204.2 MB)\nCollecting py4j==0.10.9\n  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py): started\n  Building wheel for pyspark (setup.py): finished with status &#39;done&#39;\n  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612244 sha256=13d2971b29e6f918c69e1a933e99373ada7fee8ef80e5870d11f2693b399561e\n  Stored in directory: /root/.cache/pip/wheels/5e/34/fa/b37b5cef503fc5148b478b2495043ba61b079120b7ff379f9b\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark, json5, jproperties, argparse\nSuccessfully installed argparse-1.4.0 jproperties-2.1.0 json5-0.9.5 py4j-0.10.9 pyspark-3.0.1\nPython interpreter will be restarted.\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting argparse\n  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\nCollecting jproperties\n  Downloading jproperties-2.1.0-py3-none-any.whl (17 kB)\nRequirement already satisfied: six~=1.12 in /databricks/python3/lib/python3.7/site-packages (from jproperties) (1.14.0)\nCollecting json5\n  Downloading json5-0.9.5-py2.py3-none-any.whl (17 kB)\nCollecting pyspark\n  Downloading pyspark-3.0.1.tar.gz (204.2 MB)\nCollecting py4j==0.10.9\n  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py): started\n  Building wheel for pyspark (setup.py): finished with status &#39;done&#39;\n  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612244 sha256=13d2971b29e6f918c69e1a933e99373ada7fee8ef80e5870d11f2693b399561e\n  Stored in directory: /root/.cache/pip/wheels/5e/34/fa/b37b5cef503fc5148b478b2495043ba61b079120b7ff379f9b\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark, json5, jproperties, argparse\nSuccessfully installed argparse-1.4.0 jproperties-2.1.0 json5-0.9.5 py4j-0.10.9 pyspark-3.0.1\nPython interpreter will be restarted.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%fs rm -r /tmp/stream/pykafka_ss/"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3781c8e4-f0df-4f79-a6b6-a988424cd54c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">res0: Boolean = true\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res0: Boolean = true\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.mkdirs(\"/tmp/stream/pykafka_ss/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98a37c66-035e-42d2-b55c-cd3dfd95b477"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[2]: True</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: True</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sun Aug  9 11:21:19 2020\n\n@author: sunilmiriyala\n\"\"\"\n# https://spark.apache.org/docs/latest/api/python/index.html\n\nimport sys\nimport pyspark\nfrom pyspark.streaming.kinesis import KinesisUtils, InitialPositionInStream\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.streaming import StreamingContext\nfrom jproperties import Properties\nfrom pyspark.storagelevel import StorageLevel\nfrom pyspark.sql.types import Row\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import explode\nfrom pyspark.sql.functions import split\nimport json5 as json\nfrom pyspark.sql.functions import col\n\n# https://spark.apache.org/docs/latest/api/python/index.html\n\nclass MyPySparkApp:\n    def __init__(self, **kwargs):\n        print(\"__init__::kwargs:%s\" % kwargs)\n        self.appname = kwargs.get(\"spark.name\", \"MyPySparkKafka\")\n        self.master = kwargs.get(\"spark.master\", \"local\")\n        self.batch_dur_sec = kwargs.get(\"spark.stream.batch.duration.secs\", 5)\n\n        self.src_type = kwargs.get(\"data.source.type\", \"kafka\")\n        self.src_format = kwargs.get(\"data.source.format\", \"json\")\n\n        self.kfk_topic = kwargs.get(\"data.source.kafka.topic\")\n        self.kfk_topic_out = kwargs.get(\"data.sink.kafka.topic\")\n        self.kfk_brokers = kwargs.get(\"data.source.kafka.brokers\")\n        \n        self.kfk_start = kwargs.get(\"data.source.kafka.startingposition\", \"earliest\")\n        self.kfk_chkpoint_int = kwargs.get(\"data.source.kafka.checkpointinterval\", 10)\n        self.kfk_chkpoint_dir = kwargs.get(\"data.source.kafka.checkpointdir\", \"/tmp/\")\n\n        self.kfk_auto_commit = kwargs.get(\"enable.auto.commit\", True)\n        self.sasl_protocol = kwargs.get(\"security.protocol\", \"SASL_SSL\")\n        self.sasl_mech = kwargs.get(\"sasl.mechanisms\", \"SCRAM-SHA-256\")\n        self.sasl_username = kwargs.get(\"sasl.username\")\n        self.sasl_password = kwargs.get(\"sasl.password\")\n        self.file_location = kwargs.get(\"data.lookup.location\")\n        self.file_names = kwargs.get(\"data.lookup.files\")\n\n        self.conf = SparkConf().setAppName(self.appname)\n        print(\"__init__::self.conf:%s\" % self.conf)\n        sc.setCheckpointDir(self.kfk_chkpoint_dir)\n\n        self.ssc = StreamingContext(sc, int(self.batch_dur_sec))\n        print(\"__init__::self.ssc:%s\" % (self.ssc))\n\n        self.spark = SparkSession.builder.config(conf=self.conf).getOrCreate()\n        print(\"__init__::self.spark:%s\" % self.spark)\n\n    def loadLookupTable(self):\n        if self.file_names is None:\n            print(\"loadLookupTable::file_name is none\")\n            return None\n        files = self.file_names.split(\",\")\n        lookupTableDF = self.readFile(filePath=self.file_location + files[0])\n        try:\n          lookupTableDF.createTempView(\"BROADCAST_CUST\")\n        except Exception as ex:\n          print(\"loadLookupTable::Exception:\", str(ex))\n\n    def readFile(self, filePath):\n        print(\"readFile::**********************\")\n        print(\"readFile::filePath:%s\" % filePath)\n        df = self.spark.read.format('csv').options(header='true').options(inferSchema='true').load(filePath).cache()\n        print(\"readFile::df:%s\" % (df))\n        df.printSchema()\n        df.show(truncate=True)\n        #print(\"readFile::df.count:%s\" % df.count())\n        return df\n      \n    def enrichCustId(self, input_df):\n        print(\"enrichCustId::input_df:\", input_df)\n        brd_cust_df = self.spark.sql(\"select * from BROADCAST_CUST\")\n        print(\"enrichCustId::brd_cust_df:\", brd_cust_df)\n        #input_df_new = input_df.rdd().toDF()\n        #enrichCustId::input_df: DataFrame[from_json(value): struct<id:int,cust_id:int,app_id:int,active:int>]\n        #enrichCustId::brd_cust_df: DataFrame[id: int, name: string, active: int]\n        enrich_df = input_df.join(brd_cust_df, input_df.cust_id == brd_cust_df.id, 'inner').select(brd_cust_df.name, input_df.id, input_df.app_id, input_df.active)\n        print(\"enrichCustId::enrich_df:\", enrich_df)\n        return enrich_df\n    \n\n    def readData(self):\n        print(\"readData::START2\")\n        \n        self.loadLookupTable()\n        #print(\"readData::broadcast_var:\", broadcast_var)\n        \n        cust_app_schema = StructType(\n          [ StructField(\"id\",IntegerType()), \n           StructField(\"cust_id\",IntegerType()), \n           StructField(\"app_id\",IntegerType()), \n           StructField(\"active\",IntegerType()) \n          ]\n        )\n        print(\"readData::cust_app_schema::\", cust_app_schema)\n        \n        # Read from Prop 'public.dept'\n        # https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html\n        kafka_data_df = self.spark.readStream.format(\"kafka\") \\\n            .option(\"kafka.bootstrap.servers\", self.kfk_brokers) \\\n            .option(\"subscribe\", self.kfk_topic) \\\n            .option(\"kafka.security.protocol\", self.sasl_protocol) \\\n            .option(\"kafka.sasl.mechanism\", self.sasl_mech) \\\n            .option(\"security.protocol\", self.sasl_protocol) \\\n            .option(\"sasl.mechanisms\", self.sasl_mech) \\\n            .load()\n        print(\"readData::kafka_data_df(original)::\", kafka_data_df)\n        kafka_data_df = kafka_data_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n        print(\"readData::kafka_data_df(as-string)::\", kafka_data_df)\n        cust_apps_df = kafka_data_df.select(from_json(col(\"value\"), cust_app_schema).alias(\"data\")).select(\"data.*\")\n        print(\"readData::cust_apps_df::\", cust_apps_df)\n        #cust_apps_enrich_df = cust_apps_df\n        cust_apps_enrich_df = cust_apps_df.transform(self.enrichCustId)\n        print(\"readData::cust_apps_enrich_df::\", cust_apps_enrich_df)\n        #Note that In order to write Spark Streaming data to Kafka, value column is required and all other fields are optional.\n        #columns key and value are binary in Kafka; hence, first, these should convert to String before processing. \n        #If a key column is not specified, then a null valued key column will be automatically added.\n        cust_apps_enrich_df = cust_apps_enrich_df.selectExpr(\"to_json(struct(*)) AS value\")\n        print(\"readData::final::cust_apps_enrich_df(k,v)::\", cust_apps_enrich_df)\n        cust_apps_enrich_df.writeStream.format(\"kafka\")\\\n            .option(\"checkpointLocation\", \"/tmp/stream/pykafka_ss/\")\\\n            .option(\"kafka.bootstrap.servers\", self.kfk_brokers)\\\n            .option(\"kafka.security.protocol\", self.sasl_protocol) \\\n            .option(\"kafka.sasl.mechanism\", self.sasl_mech) \\\n            .option(\"security.protocol\", self.sasl_protocol) \\\n            .option(\"sasl.mechanisms\", self.sasl_mech) \\\n            .option(\"topic\", self.kfk_topic_out)\\\n            .start()\\\n            .awaitTermination()\n        \n\n    def start(self):\n        self.readData()\n\n\n#(\"kafka.bootstrap.servers\", \"omnibus-01.srvs.cloudkafka.com:9094,omnibus-02.srvs.cloudkafka.com:9094,omnibus-03.srvs.cloudkafka.com:9094\") \\\n#(\"kafka.security.protocol\", \"SASL_SSL\") \\\n#(\"kafka.sasl.mechanism\", \"SCRAM-SHA-256\") \\\n#(\"security.protocol\", \"SASL_SSL\") \\\n#(\"sasl.mechanism\", \"SCRAM-SHA-256\") \\\n#(\"subscribe\", \"yrmfqh3q-test\") \\\nif __name__ == '__main__':\n    kw = {\n        \"spark.master\": \"local\",\n        \"spark.name\": \"MyPySpark-Kafka\",\n        \"spark.stream.batch.duration.secs\": \"10\",\n        \"data.source.type\": \"kafka\",\n        \"data.source.format\": \"json\",\n        \"data.source.kafka.topic\": \"yrmfqh3q-test\",\n        \"data.sink.kafka.topic\": \"yrmfqh3q-default\",\n        \"data.source.kafka.group\": \"pyspark-kafka\",\n        \"data.source.kafka.brokers\": \"omnibus-01.srvs.cloudkafka.com:9094,omnibus-03.srvs.cloudkafka.com:9094,omnibus-02.srvs.cloudkafka.com:9094\",\n        \"data.source.kafka.startingposition\": \"earliest\",\n        \"data.source.kafka.checkpointinterval\": \"5\",\n        \"security.protocol\": \"SASL_SSL\",\n        \"sasl.mechanisms\": \"SCRAM-SHA-256\",\n        \"sasl.username\": \"yrmfqh3q\",\n        \"sasl.password\": \"WfKJEYL_hwNUHVU8laSbj6gXkPIw_xuc\",\n        \"checkpointdir\": \"/tmp/stream/pykafka_ss/\",\n        \"data.lookup.location\": \"dbfs:/FileStore/tables/\",\n        \"data.lookup.files\": \"customers.csv\"\n    }\n    print(\"kw::%s\" % kw)\n    app = MyPySparkApp(**kw)\n    app.start()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"abed09ed-6069-469f-9b36-e57dd4a8a053"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">kw::{&#39;spark.master&#39;: &#39;local&#39;, &#39;spark.name&#39;: &#39;MyPySpark-Kafka&#39;, &#39;spark.stream.batch.duration.secs&#39;: &#39;10&#39;, &#39;data.source.type&#39;: &#39;kafka&#39;, &#39;data.source.format&#39;: &#39;json&#39;, &#39;data.source.kafka.topic&#39;: &#39;yrmfqh3q-test&#39;, &#39;data.sink.kafka.topic&#39;: &#39;yrmfqh3q-default&#39;, &#39;data.source.kafka.group&#39;: &#39;pyspark-kafka&#39;, &#39;data.source.kafka.brokers&#39;: &#39;omnibus-01.srvs.cloudkafka.com:9094,omnibus-03.srvs.cloudkafka.com:9094,omnibus-02.srvs.cloudkafka.com:9094&#39;, &#39;data.source.kafka.startingposition&#39;: &#39;earliest&#39;, &#39;data.source.kafka.checkpointinterval&#39;: &#39;5&#39;, &#39;security.protocol&#39;: &#39;SASL_SSL&#39;, &#39;sasl.mechanisms&#39;: &#39;SCRAM-SHA-256&#39;, &#39;sasl.username&#39;: &#39;yrmfqh3q&#39;, &#39;sasl.password&#39;: &#39;WfKJEYL_hwNUHVU8laSbj6gXkPIw_xuc&#39;, &#39;checkpointdir&#39;: &#39;/tmp/stream/pykafka_ss/&#39;, &#39;data.lookup.location&#39;: &#39;dbfs:/FileStore/tables/&#39;, &#39;data.lookup.files&#39;: &#39;customers.csv&#39;}\n__init__::kwargs:{&#39;spark.master&#39;: &#39;local&#39;, &#39;spark.name&#39;: &#39;MyPySpark-Kafka&#39;, &#39;spark.stream.batch.duration.secs&#39;: &#39;10&#39;, &#39;data.source.type&#39;: &#39;kafka&#39;, &#39;data.source.format&#39;: &#39;json&#39;, &#39;data.source.kafka.topic&#39;: &#39;yrmfqh3q-test&#39;, &#39;data.sink.kafka.topic&#39;: &#39;yrmfqh3q-default&#39;, &#39;data.source.kafka.group&#39;: &#39;pyspark-kafka&#39;, &#39;data.source.kafka.brokers&#39;: &#39;omnibus-01.srvs.cloudkafka.com:9094,omnibus-03.srvs.cloudkafka.com:9094,omnibus-02.srvs.cloudkafka.com:9094&#39;, &#39;data.source.kafka.startingposition&#39;: &#39;earliest&#39;, &#39;data.source.kafka.checkpointinterval&#39;: &#39;5&#39;, &#39;security.protocol&#39;: &#39;SASL_SSL&#39;, &#39;sasl.mechanisms&#39;: &#39;SCRAM-SHA-256&#39;, &#39;sasl.username&#39;: &#39;yrmfqh3q&#39;, &#39;sasl.password&#39;: &#39;WfKJEYL_hwNUHVU8laSbj6gXkPIw_xuc&#39;, &#39;checkpointdir&#39;: &#39;/tmp/stream/pykafka_ss/&#39;, &#39;data.lookup.location&#39;: &#39;dbfs:/FileStore/tables/&#39;, &#39;data.lookup.files&#39;: &#39;customers.csv&#39;}\n__init__::self.conf:&lt;pyspark.conf.SparkConf object at 0x7fb95540f290&gt;\n__init__::self.ssc:&lt;pyspark.streaming.context.StreamingContext object at 0x7fb9581ee910&gt;\n__init__::self.spark:&lt;pyspark.sql.session.SparkSession object at 0x7fb9563984d0&gt;\nreadData::START2\nreadFile::**********************\nreadFile::filePath:dbfs:/FileStore/tables/customers.csv\nreadFile::df:DataFrame[id: int, name: string, active: int]\nroot\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- active: integer (nullable = true)\n\n+---+-----+------+\n| id| name|active|\n+---+-----+------+\n|  1|Cust1|     1|\n|  2|Cust2|     0|\n|  3|Cust3|     1|\n+---+-----+------+\n\nloadLookupTable::Exception: Temporary view &#39;BROADCAST_CUST&#39; already exists;\nreadData::cust_app_schema:: StructType(List(StructField(id,IntegerType,true),StructField(cust_id,IntegerType,true),StructField(app_id,IntegerType,true),StructField(active,IntegerType,true)))\nreadData::kafka_data_df(original):: DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]\nreadData::kafka_data_df(as-string):: DataFrame[key: string, value: string]\nreadData::cust_apps_df:: DataFrame[id: int, cust_id: int, app_id: int, active: int]\nenrichCustId::input_df: DataFrame[id: int, cust_id: int, app_id: int, active: int]\nenrichCustId::brd_cust_df: DataFrame[id: int, name: string, active: int]\nenrichCustId::enrich_df: DataFrame[name: string, id: int, app_id: int, active: int]\nreadData::cust_apps_enrich_df:: DataFrame[name: string, id: int, app_id: int, active: int]\nreadData::final::cust_apps_enrich_df(k,v):: DataFrame[value: string]\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">kw::{&#39;spark.master&#39;: &#39;local&#39;, &#39;spark.name&#39;: &#39;MyPySpark-Kafka&#39;, &#39;spark.stream.batch.duration.secs&#39;: &#39;10&#39;, &#39;data.source.type&#39;: &#39;kafka&#39;, &#39;data.source.format&#39;: &#39;json&#39;, &#39;data.source.kafka.topic&#39;: &#39;yrmfqh3q-test&#39;, &#39;data.sink.kafka.topic&#39;: &#39;yrmfqh3q-default&#39;, &#39;data.source.kafka.group&#39;: &#39;pyspark-kafka&#39;, &#39;data.source.kafka.brokers&#39;: &#39;omnibus-01.srvs.cloudkafka.com:9094,omnibus-03.srvs.cloudkafka.com:9094,omnibus-02.srvs.cloudkafka.com:9094&#39;, &#39;data.source.kafka.startingposition&#39;: &#39;earliest&#39;, &#39;data.source.kafka.checkpointinterval&#39;: &#39;5&#39;, &#39;security.protocol&#39;: &#39;SASL_SSL&#39;, &#39;sasl.mechanisms&#39;: &#39;SCRAM-SHA-256&#39;, &#39;sasl.username&#39;: &#39;yrmfqh3q&#39;, &#39;sasl.password&#39;: &#39;WfKJEYL_hwNUHVU8laSbj6gXkPIw_xuc&#39;, &#39;checkpointdir&#39;: &#39;/tmp/stream/pykafka_ss/&#39;, &#39;data.lookup.location&#39;: &#39;dbfs:/FileStore/tables/&#39;, &#39;data.lookup.files&#39;: &#39;customers.csv&#39;}\n__init__::kwargs:{&#39;spark.master&#39;: &#39;local&#39;, &#39;spark.name&#39;: &#39;MyPySpark-Kafka&#39;, &#39;spark.stream.batch.duration.secs&#39;: &#39;10&#39;, &#39;data.source.type&#39;: &#39;kafka&#39;, &#39;data.source.format&#39;: &#39;json&#39;, &#39;data.source.kafka.topic&#39;: &#39;yrmfqh3q-test&#39;, &#39;data.sink.kafka.topic&#39;: &#39;yrmfqh3q-default&#39;, &#39;data.source.kafka.group&#39;: &#39;pyspark-kafka&#39;, &#39;data.source.kafka.brokers&#39;: &#39;omnibus-01.srvs.cloudkafka.com:9094,omnibus-03.srvs.cloudkafka.com:9094,omnibus-02.srvs.cloudkafka.com:9094&#39;, &#39;data.source.kafka.startingposition&#39;: &#39;earliest&#39;, &#39;data.source.kafka.checkpointinterval&#39;: &#39;5&#39;, &#39;security.protocol&#39;: &#39;SASL_SSL&#39;, &#39;sasl.mechanisms&#39;: &#39;SCRAM-SHA-256&#39;, &#39;sasl.username&#39;: &#39;yrmfqh3q&#39;, &#39;sasl.password&#39;: &#39;WfKJEYL_hwNUHVU8laSbj6gXkPIw_xuc&#39;, &#39;checkpointdir&#39;: &#39;/tmp/stream/pykafka_ss/&#39;, &#39;data.lookup.location&#39;: &#39;dbfs:/FileStore/tables/&#39;, &#39;data.lookup.files&#39;: &#39;customers.csv&#39;}\n__init__::self.conf:&lt;pyspark.conf.SparkConf object at 0x7fb95540f290&gt;\n__init__::self.ssc:&lt;pyspark.streaming.context.StreamingContext object at 0x7fb9581ee910&gt;\n__init__::self.spark:&lt;pyspark.sql.session.SparkSession object at 0x7fb9563984d0&gt;\nreadData::START2\nreadFile::**********************\nreadFile::filePath:dbfs:/FileStore/tables/customers.csv\nreadFile::df:DataFrame[id: int, name: string, active: int]\nroot\n-- id: integer (nullable = true)\n-- name: string (nullable = true)\n-- active: integer (nullable = true)\n\n+---+-----+------+\n id| name|active|\n+---+-----+------+\n  1|Cust1|     1|\n  2|Cust2|     0|\n  3|Cust3|     1|\n+---+-----+------+\n\nloadLookupTable::Exception: Temporary view &#39;BROADCAST_CUST&#39; already exists;\nreadData::cust_app_schema:: StructType(List(StructField(id,IntegerType,true),StructField(cust_id,IntegerType,true),StructField(app_id,IntegerType,true),StructField(active,IntegerType,true)))\nreadData::kafka_data_df(original):: DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]\nreadData::kafka_data_df(as-string):: DataFrame[key: string, value: string]\nreadData::cust_apps_df:: DataFrame[id: int, cust_id: int, app_id: int, active: int]\nenrichCustId::input_df: DataFrame[id: int, cust_id: int, app_id: int, active: int]\nenrichCustId::brd_cust_df: DataFrame[id: int, name: string, active: int]\nenrichCustId::enrich_df: DataFrame[name: string, id: int, app_id: int, active: int]\nreadData::cust_apps_enrich_df:: DataFrame[name: string, id: int, app_id: int, active: int]\nreadData::final::cust_apps_enrich_df(k,v):: DataFrame[value: string]\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Cancelled","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sun Aug  9 11:21:19 2020\n\n@author: sunilmiriyala\n\"\"\"\n# https://spark.apache.org/docs/latest/api/python/index.html\n\nimport sys\nimport pyspark\nfrom pyspark.streaming.kinesis import KinesisUtils, InitialPositionInStream\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.streaming import StreamingContext\nfrom jproperties import Properties\nfrom pyspark.storagelevel import StorageLevel\nfrom pyspark.sql.types import Row\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import explode\nfrom pyspark.sql.functions import split\nimport json5 as json\nfrom pyspark.sql.functions import col\n\n# https://spark.apache.org/docs/latest/api/python/index.html\n\nclass MyPySparkApp:\n    def __init__(self, **kwargs):\n        print(\"__init__::kwargs:%s\" % kwargs)\n        self.appname = kwargs.get(\"spark.name\", \"MyPySparkKafka\")\n        self.master = kwargs.get(\"spark.master\", \"local\")\n        self.batch_dur_sec = kwargs.get(\"spark.stream.batch.duration.secs\", 5)\n\n        self.src_type = kwargs.get(\"data.source.type\", \"kafka\")\n        self.src_format = kwargs.get(\"data.source.format\", \"json\")\n\n        self.kfk_topic = kwargs.get(\"data.source.kafka.topic\")\n        self.kfk_topic_out = kwargs.get(\"data.sink.kafka.topic\")\n        self.kfk_brokers = kwargs.get(\"data.source.kafka.brokers\")\n        \n        self.kfk_start = kwargs.get(\"data.source.kafka.startingposition\", \"earliest\")\n        self.kfk_chkpoint_int = kwargs.get(\"data.source.kafka.checkpointinterval\", 10)\n        self.kfk_chkpoint_dir = kwargs.get(\"data.source.kafka.checkpointdir\", \"/tmp/\")\n\n        self.kfk_auto_commit = kwargs.get(\"enable.auto.commit\", True)\n        self.sasl_protocol = kwargs.get(\"security.protocol\", \"SASL_SSL\")\n        self.sasl_mech = kwargs.get(\"sasl.mechanisms\", \"SCRAM-SHA-256\")\n        self.sasl_username = kwargs.get(\"sasl.username\")\n        self.sasl_password = kwargs.get(\"sasl.password\")\n        self.file_location = kwargs.get(\"data.lookup.location\")\n        self.file_names = kwargs.get(\"data.lookup.files\")\n\n        self.conf = SparkConf().setAppName(self.appname)\n        print(\"__init__::self.conf:%s\" % self.conf)\n        sc.setCheckpointDir(self.kfk_chkpoint_dir)\n\n        self.ssc = StreamingContext(sc, int(self.batch_dur_sec))\n        print(\"__init__::self.ssc:%s\" % (self.ssc))\n\n        self.spark = SparkSession.builder.config(conf=self.conf).getOrCreate()\n        print(\"__init__::self.spark:%s\" % self.spark)\n\n    def loadLookupTable(self):\n        if self.file_names is None:\n            print(\"loadLookupTable::file_name is none\")\n            return None\n        files = self.file_names.split(\",\")\n        lookupTableDF = self.readFile(filePath=self.file_location + files[0])\n        pdf_j = lookupTableDF.toJSON().collect()\n        print(\"readData::pdf_j:%s\" % pdf_j)\n        broadcast_var = sc.broadcast(json.loads(pdf_j))\n        print(\"readData::broadcast_var:\", broadcast_var, broadcast_var.value)\n        return broadcast_var\n\n    def readFile(self, filePath):\n        print(\"readFile::**********************\")\n        print(\"readFile::filePath:%s\" % filePath)\n        df = self.spark.read.format('csv').options(header='true').options(inferSchema='true').load(filePath).cache()\n        print(\"readFile::df:%s\" % (df))\n        df.printSchema()\n        df.show(truncate=True)\n        #print(\"readFile::df.count:%s\" % df.count())\n        return df\n\n    def enrichCustId(self, input_df):\n        print(\"enrichCustId:\", type(input_df), input_df)\n        vaf = input_df[\"value\"]\n        return input_df\n\n    def readData(self):\n        print(\"readData\")\n        broadcast_var = self.loadLookupTable()\n\n        # Read from Prop 'public.dept'\n        # https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html\n        kafka_data_df = self.spark.readStream.format(\"kafka\") \\\n            .option(\"kafka.bootstrap.servers\", self.kfk_brokers) \\\n            .option(\"subscribe\", self.kfk_topic) \\\n            .option(\"kafka.security.protocol\", self.sasl_protocol) \\\n            .option(\"kafka.sasl.mechanism\", self.sasl_mech) \\\n            .option(\"security.protocol\", self.sasl_protocol) \\\n            .option(\"sasl.mechanisms\", self.sasl_mech) \\\n            .load()\n        print(\"readData::kafka_data_df(1)::\", kafka_data_df)\n        kafka_data_df = kafka_data_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n        print(\"readData::kafka_data_df(2)::\", type(kafka_data_df), kafka_data_df)\n        \n        #kafka_data_df.transform(self.enrichCustId)\n        \n        kafka_data_df.writeStream.format(\"kafka\")\\\n            .option(\"checkpointLocation\", \"/tmp/stream/pykafka_ss/\")\\\n            .option(\"kafka.bootstrap.servers\", self.kfk_brokers)\\\n            .option(\"kafka.security.protocol\", self.sasl_protocol) \\\n            .option(\"kafka.sasl.mechanism\", self.sasl_mech) \\\n            .option(\"security.protocol\", self.sasl_protocol) \\\n            .option(\"sasl.mechanisms\", self.sasl_mech) \\\n            .option(\"topic\", self.kfk_topic_out)\\\n            .start()\\\n            .awaitTermination()\n\n    def start(self):\n        self.readData()\n\n\n#(\"kafka.bootstrap.servers\", \"omnibus-01.srvs.cloudkafka.com:9094,omnibus-02.srvs.cloudkafka.com:9094,omnibus-03.srvs.cloudkafka.com:9094\") \\\n#(\"kafka.security.protocol\", \"SASL_SSL\") \\\n#(\"kafka.sasl.mechanism\", \"SCRAM-SHA-256\") \\\n#(\"security.protocol\", \"SASL_SSL\") \\\n#(\"sasl.mechanism\", \"SCRAM-SHA-256\") \\\n#(\"subscribe\", \"yrmfqh3q-test\") \\\nif __name__ == '__main__':\n    kw = {\n        \"spark.master\": \"local\",\n        \"spark.name\": \"MyPySpark-Kafka\",\n        \"spark.stream.batch.duration.secs\": \"10\",\n        \"data.source.type\": \"kafka\",\n        \"data.source.format\": \"json\",\n        \"data.source.kafka.topic\": \"yrmfqh3q-test\",\n        \"data.sink.kafka.topic\": \"yrmfqh3q-default\",\n        \"data.source.kafka.group\": \"pyspark-kafka\",\n        \"data.source.kafka.brokers\": \"omnibus-01.srvs.cloudkafka.com:9094,omnibus-03.srvs.cloudkafka.com:9094,omnibus-02.srvs.cloudkafka.com:9094\",\n        \"data.source.kafka.startingposition\": \"earliest\",\n        \"data.source.kafka.checkpointinterval\": \"5\",\n        \"security.protocol\": \"SASL_SSL\",\n        \"sasl.mechanisms\": \"SCRAM-SHA-256\",\n        \"sasl.username\": \"yrmfqh3q\",\n        \"sasl.password\": \"WfKJEYL_hwNUHVU8laSbj6gXkPIw_xuc\",\n        \"checkpointdir\": \"/tmp/stream/pykafka_ss/\",\n        \"data.lookup.location\": \"dbfs:/FileStore/tables/\",\n        \"data.lookup.files\": \"customers.csv\"\n    }\n    print(\"kw::%s\" % kw)\n    app = MyPySparkApp(**kw)\n    app.start()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb83bb73-4e36-4f80-8a46-a85f6cdf1f92"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">kw::{&#39;spark.master&#39;: &#39;local&#39;, &#39;spark.name&#39;: &#39;MyPySpark-Kafka&#39;, &#39;spark.stream.batch.duration.secs&#39;: &#39;10&#39;, &#39;data.source.type&#39;: &#39;kafka&#39;, &#39;data.source.format&#39;: &#39;json&#39;, &#39;data.source.kafka.topic&#39;: &#39;yrmfqh3q-test&#39;, &#39;data.sink.kafka.topic&#39;: &#39;yrmfqh3q-default&#39;, &#39;data.source.kafka.group&#39;: &#39;pyspark-kafka&#39;, &#39;data.source.kafka.brokers&#39;: &#39;omnibus-01.srvs.cloudkafka.com:9094,omnibus-03.srvs.cloudkafka.com:9094,omnibus-02.srvs.cloudkafka.com:9094&#39;, &#39;data.source.kafka.startingposition&#39;: &#39;earliest&#39;, &#39;data.source.kafka.checkpointinterval&#39;: &#39;5&#39;, &#39;security.protocol&#39;: &#39;SASL_SSL&#39;, &#39;sasl.mechanisms&#39;: &#39;SCRAM-SHA-256&#39;, &#39;sasl.username&#39;: &#39;yrmfqh3q&#39;, &#39;sasl.password&#39;: &#39;WfKJEYL_hwNUHVU8laSbj6gXkPIw_xuc&#39;, &#39;checkpointdir&#39;: &#39;/tmp/stream/pykafka_ss/&#39;, &#39;data.lookup.location&#39;: &#39;dbfs:/FileStore/tables/&#39;, &#39;data.lookup.files&#39;: &#39;customers.csv&#39;}\n__init__::kwargs:{&#39;spark.master&#39;: &#39;local&#39;, &#39;spark.name&#39;: &#39;MyPySpark-Kafka&#39;, &#39;spark.stream.batch.duration.secs&#39;: &#39;10&#39;, &#39;data.source.type&#39;: &#39;kafka&#39;, &#39;data.source.format&#39;: &#39;json&#39;, &#39;data.source.kafka.topic&#39;: &#39;yrmfqh3q-test&#39;, &#39;data.sink.kafka.topic&#39;: &#39;yrmfqh3q-default&#39;, &#39;data.source.kafka.group&#39;: &#39;pyspark-kafka&#39;, &#39;data.source.kafka.brokers&#39;: &#39;omnibus-01.srvs.cloudkafka.com:9094,omnibus-03.srvs.cloudkafka.com:9094,omnibus-02.srvs.cloudkafka.com:9094&#39;, &#39;data.source.kafka.startingposition&#39;: &#39;earliest&#39;, &#39;data.source.kafka.checkpointinterval&#39;: &#39;5&#39;, &#39;security.protocol&#39;: &#39;SASL_SSL&#39;, &#39;sasl.mechanisms&#39;: &#39;SCRAM-SHA-256&#39;, &#39;sasl.username&#39;: &#39;yrmfqh3q&#39;, &#39;sasl.password&#39;: &#39;WfKJEYL_hwNUHVU8laSbj6gXkPIw_xuc&#39;, &#39;checkpointdir&#39;: &#39;/tmp/stream/pykafka_ss/&#39;, &#39;data.lookup.location&#39;: &#39;dbfs:/FileStore/tables/&#39;, &#39;data.lookup.files&#39;: &#39;customers.csv&#39;}\n__init__::self.conf:&lt;pyspark.conf.SparkConf object at 0x7fdc8f4e7e90&gt;\n__init__::self.ssc:&lt;pyspark.streaming.context.StreamingContext object at 0x7fdc8f4e7d90&gt;\n__init__::self.spark:&lt;pyspark.sql.session.SparkSession object at 0x7fdc8d68e490&gt;\nreadData\nreadFile::**********************\nreadFile::filePath:dbfs:/FileStore/tables/customers.csv\nreadFile::df:DataFrame[id: int, name: string, active: int]\nroot\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- active: integer (nullable = true)\n\n+---+-----+------+\n| id| name|active|\n+---+-----+------+\n|  1|Cust1|     1|\n|  2|Cust2|     0|\n|  3|Cust3|     1|\n+---+-----+------+\n\nreadData::pdf_j:[&#39;{&#34;id&#34;:1,&#34;name&#34;:&#34;Cust1&#34;,&#34;active&#34;:1}&#39;, &#39;{&#34;id&#34;:2,&#34;name&#34;:&#34;Cust2&#34;,&#34;active&#34;:0}&#39;, &#39;{&#34;id&#34;:3,&#34;name&#34;:&#34;Cust3&#34;,&#34;active&#34;:1}&#39;]\nreadData::broadcast_var: &lt;pyspark.broadcast.Broadcast object at 0x7fdc87d7a610&gt; [&#39;{&#34;id&#34;:1,&#34;name&#34;:&#34;Cust1&#34;,&#34;active&#34;:1}&#39;, &#39;{&#34;id&#34;:2,&#34;name&#34;:&#34;Cust2&#34;,&#34;active&#34;:0}&#39;, &#39;{&#34;id&#34;:3,&#34;name&#34;:&#34;Cust3&#34;,&#34;active&#34;:1}&#39;]\nreadData::kafka_data_df(1):: DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]\nreadData::kafka_data_df(2):: &lt;class &#39;pyspark.sql.dataframe.DataFrame&#39;&gt; DataFrame[key: string, value: string]\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">kw::{&#39;spark.master&#39;: &#39;local&#39;, &#39;spark.name&#39;: &#39;MyPySpark-Kafka&#39;, &#39;spark.stream.batch.duration.secs&#39;: &#39;10&#39;, &#39;data.source.type&#39;: &#39;kafka&#39;, &#39;data.source.format&#39;: &#39;json&#39;, &#39;data.source.kafka.topic&#39;: &#39;yrmfqh3q-test&#39;, &#39;data.sink.kafka.topic&#39;: &#39;yrmfqh3q-default&#39;, &#39;data.source.kafka.group&#39;: &#39;pyspark-kafka&#39;, &#39;data.source.kafka.brokers&#39;: &#39;omnibus-01.srvs.cloudkafka.com:9094,omnibus-03.srvs.cloudkafka.com:9094,omnibus-02.srvs.cloudkafka.com:9094&#39;, &#39;data.source.kafka.startingposition&#39;: &#39;earliest&#39;, &#39;data.source.kafka.checkpointinterval&#39;: &#39;5&#39;, &#39;security.protocol&#39;: &#39;SASL_SSL&#39;, &#39;sasl.mechanisms&#39;: &#39;SCRAM-SHA-256&#39;, &#39;sasl.username&#39;: &#39;yrmfqh3q&#39;, &#39;sasl.password&#39;: &#39;WfKJEYL_hwNUHVU8laSbj6gXkPIw_xuc&#39;, &#39;checkpointdir&#39;: &#39;/tmp/stream/pykafka_ss/&#39;, &#39;data.lookup.location&#39;: &#39;dbfs:/FileStore/tables/&#39;, &#39;data.lookup.files&#39;: &#39;customers.csv&#39;}\n__init__::kwargs:{&#39;spark.master&#39;: &#39;local&#39;, &#39;spark.name&#39;: &#39;MyPySpark-Kafka&#39;, &#39;spark.stream.batch.duration.secs&#39;: &#39;10&#39;, &#39;data.source.type&#39;: &#39;kafka&#39;, &#39;data.source.format&#39;: &#39;json&#39;, &#39;data.source.kafka.topic&#39;: &#39;yrmfqh3q-test&#39;, &#39;data.sink.kafka.topic&#39;: &#39;yrmfqh3q-default&#39;, &#39;data.source.kafka.group&#39;: &#39;pyspark-kafka&#39;, &#39;data.source.kafka.brokers&#39;: &#39;omnibus-01.srvs.cloudkafka.com:9094,omnibus-03.srvs.cloudkafka.com:9094,omnibus-02.srvs.cloudkafka.com:9094&#39;, &#39;data.source.kafka.startingposition&#39;: &#39;earliest&#39;, &#39;data.source.kafka.checkpointinterval&#39;: &#39;5&#39;, &#39;security.protocol&#39;: &#39;SASL_SSL&#39;, &#39;sasl.mechanisms&#39;: &#39;SCRAM-SHA-256&#39;, &#39;sasl.username&#39;: &#39;yrmfqh3q&#39;, &#39;sasl.password&#39;: &#39;WfKJEYL_hwNUHVU8laSbj6gXkPIw_xuc&#39;, &#39;checkpointdir&#39;: &#39;/tmp/stream/pykafka_ss/&#39;, &#39;data.lookup.location&#39;: &#39;dbfs:/FileStore/tables/&#39;, &#39;data.lookup.files&#39;: &#39;customers.csv&#39;}\n__init__::self.conf:&lt;pyspark.conf.SparkConf object at 0x7fdc8f4e7e90&gt;\n__init__::self.ssc:&lt;pyspark.streaming.context.StreamingContext object at 0x7fdc8f4e7d90&gt;\n__init__::self.spark:&lt;pyspark.sql.session.SparkSession object at 0x7fdc8d68e490&gt;\nreadData\nreadFile::**********************\nreadFile::filePath:dbfs:/FileStore/tables/customers.csv\nreadFile::df:DataFrame[id: int, name: string, active: int]\nroot\n-- id: integer (nullable = true)\n-- name: string (nullable = true)\n-- active: integer (nullable = true)\n\n+---+-----+------+\n id| name|active|\n+---+-----+------+\n  1|Cust1|     1|\n  2|Cust2|     0|\n  3|Cust3|     1|\n+---+-----+------+\n\nreadData::pdf_j:[&#39;{&#34;id&#34;:1,&#34;name&#34;:&#34;Cust1&#34;,&#34;active&#34;:1}&#39;, &#39;{&#34;id&#34;:2,&#34;name&#34;:&#34;Cust2&#34;,&#34;active&#34;:0}&#39;, &#39;{&#34;id&#34;:3,&#34;name&#34;:&#34;Cust3&#34;,&#34;active&#34;:1}&#39;]\nreadData::broadcast_var: &lt;pyspark.broadcast.Broadcast object at 0x7fdc87d7a610&gt; [&#39;{&#34;id&#34;:1,&#34;name&#34;:&#34;Cust1&#34;,&#34;active&#34;:1}&#39;, &#39;{&#34;id&#34;:2,&#34;name&#34;:&#34;Cust2&#34;,&#34;active&#34;:0}&#39;, &#39;{&#34;id&#34;:3,&#34;name&#34;:&#34;Cust3&#34;,&#34;active&#34;:1}&#39;]\nreadData::kafka_data_df(1):: DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]\nreadData::kafka_data_df(2):: &lt;class &#39;pyspark.sql.dataframe.DataFrame&#39;&gt; DataFrame[key: string, value: string]\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Cancelled","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-kafka","dashboards":[],"language":"python","widgets":{},"notebookOrigID":520893789798990}},"nbformat":4,"nbformat_minor":0}
